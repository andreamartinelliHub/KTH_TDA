{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import functools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import gudhi as gd\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64) \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I WANT TO COMPARE CLOUDS WITH 1K POINTS WITH THE SAME STARTING POINT\n",
    "def generate_orbits(n_points_per_orbit = 1000, params = [2.5, 3.5, 4.0, 4.1, 4.3], same_init_point = True):\n",
    "    # create point clouds \n",
    "    ORBITS = np.zeros([len(params), n_points_per_orbit, 2])\n",
    "    xcur_0, ycur_0 = np.random.rand(), np.random.rand() # not necesary to save the first one\n",
    "    for id_pc, param in enumerate(params): # id_point_cloud\n",
    "        if same_init_point:\n",
    "            xcur, ycur = xcur_0, ycur_0 # not necesary to save the first one\n",
    "        else:\n",
    "            xcur, ycur =np.random.rand(), np.random.rand()\n",
    "        for id_pt in range(n_points_per_orbit): # id_point\n",
    "            xcur = (xcur + param * ycur * (1. - ycur)) % 1\n",
    "            ycur = (ycur + param * xcur * (1. - xcur)) % 1\n",
    "            ORBITS[id_pc, id_pt, :] = [xcur, ycur]\n",
    "    return ORBITS\n",
    "\n",
    "# function from [len(params), n_points, 2] to alpha-complex and persistence diagram\n",
    "# to create PDs we need to: points -> skeleton(ac) -> simplex(st) -> persistence(pers)\n",
    "# for each element of the dataset we'll have len(params) PDs to be compared\n",
    "def extract_PD(cloud, id_class):\n",
    "    \"\"\"extract a dict \n",
    "\n",
    "    Args:\n",
    "        cloud (_type_): array [1000,2] composing th ewhole point cloud\n",
    "        id_class (_type_): index about the class of membership\n",
    "\n",
    "    Returns:\n",
    "        dict: with keys ['persist_0','persist_1','id_class']\n",
    "    \"\"\"\n",
    "    # for every point cloud we create a dictionary storing the label and its persistence\n",
    "    # usage of dictionary to store each other possible data linked to the point clous\n",
    "    ac = gd.AlphaComplex(points=cloud)\n",
    "    st = ac.create_simplex_tree()\n",
    "    pers = st.persistence()\n",
    "    \n",
    "    #! TRANSPOSE TO HAVE THEN [BATCH SIZE, 2, NUM POINTS]\n",
    "    pers_0 = np.array(st.persistence_intervals_in_dimension(0)).transpose()\n",
    "    pers_1 = np.array(st.persistence_intervals_in_dimension(1)).transpose()\n",
    "    pers_dict = {\n",
    "        'persist_0': pers_0[:,:-1], # removing the last barcode, the one with inf\n",
    "        'persist_1': pers_1, # here we should never have inf, since [0,1]^2 is compact/bounded  \n",
    "        'persist': pers, # actual PD\n",
    "        'id_class': id_class # label for classification\n",
    "    }\n",
    "    return pers_dict\n",
    "\n",
    "def gaussian_transformation(pd):\n",
    "    # Apply the function along the rows\n",
    "    embs = np.apply_along_axis(gamma_p, axis=1, arr=pd)\n",
    "    return torch.tensor(embs)\n",
    "\n",
    "def gamma_p(p):\n",
    "    # params of gaussian_transformation\n",
    "    ts = torch.tensor([[0., 0.], \\\n",
    "          [0.25, 0.], [0.25, 0.25], \\\n",
    "          [0.5, 0.], [0.5, 0.25], [0.5, 0.5], \\\n",
    "          [0.75, 0.], [0.75, 0.25], [0.75, 0.5], [0.75, 0.75], \\\n",
    "          [1., 0.], [1., 0.25], [1., 0.5], [1., 0.75], [1., 1.]])\n",
    "    sigma = 0.2\n",
    "    # single point computaions for gaussian transformation\n",
    "    squared_distances = torch.pow(ts - p, 2).sum(dim=1)\n",
    "    emb = -squared_distances/(2*sigma**2)\n",
    "    emb = torch.exp(emb)\n",
    "    return emb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create TRAIN Point Clouds:   0%|          | 1/700 [00:00<01:43,  6.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create TRAIN Point Clouds: 100%|██████████| 700/700 [01:41<00:00,  6.87it/s]\n",
      "Create TEST Point Clouds: 100%|██████████| 300/300 [00:45<00:00,  6.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# hyper params\n",
    "n_points = 1000\n",
    "params = [2.5, 3.5, 4.0, 4.1, 4.3]\n",
    "same_init_point = True\n",
    "n_seq_per_dataset = [700, 300] # [size train, size test] for each class\n",
    "\n",
    "extended_pers = False # to be implemented, at the moment not interested in\n",
    "\n",
    "# init list fo persistence diagrams\n",
    "pds_train = []\n",
    "pds_test = []\n",
    "\n",
    "# TRAIN \n",
    "for i in tqdm(range(n_seq_per_dataset[0]), desc='Create TRAIN Point Clouds'):\n",
    "    ORBS = generate_orbits(n_points, params, same_init_point) # CREATE THE 5 POINT CLOUDS\n",
    "    for j in range(ORBS.shape[0]):\n",
    "        ij_pers = extract_PD(ORBS[j,:,:], j) # EXTRACT PDs\n",
    "        pds_train.append(ij_pers) # STORE IN THE LIST pds_train\n",
    "\n",
    "# TEST\n",
    "for i in tqdm(range(n_seq_per_dataset[1]), desc='Create TEST Point Clouds'):\n",
    "    ORBS = generate_orbits(n_points, params, same_init_point) # CREATE THE 5 POINT CLOUDS\n",
    "    for j in range(ORBS.shape[0]):\n",
    "        ij_pers = extract_PD(ORBS[j,:,:], j) # EXTRACT PDs\n",
    "        pds_test.append(ij_pers) # STORE IN THE LIST pds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(pds_train[0])\n",
    "# pds_train[0]['persist_0'] # H0 of a Pers Diagram\n",
    "# len(pds_train[0]['persist_0']) # H0 of a Pers Diagram: [[births...], [deaths...]]\n",
    "\n",
    "num_dgs_train = len(pds_train)\n",
    "num_dgs_test = len(pds_test)\n",
    "\n",
    "max_size_H0 = 999\n",
    "max_size_train_H1 = max([len(pds_train[i]['persist_1'][0]) for i in range(num_dgs_train)])\n",
    "max_size_test_H1 = max([len(pds_test[i]['persist_1'][0]) for i in range(num_dgs_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHICH TRANSFORMATION DO WE WANT TO APPLY TO PDs?\n",
    "# todo: HERE I WANT ALSO TO APPLY A DENSITY IN SOME CASES \n",
    "\n",
    "METHOD = 'Linear_length'\n",
    "preproc_size = 750\n",
    "\n",
    "if METHOD == 'Linear_length':\n",
    "    \n",
    "    train_vecs_H0 = np.zeros((num_dgs_train, max_size_H0))\n",
    "    train_vecs_H1 = np.zeros((num_dgs_train, max_size_train_H1))\n",
    "    test_vecs_H0 = np.zeros((num_dgs_test, max_size_H0))\n",
    "    test_vecs_H1 = np.zeros((num_dgs_test, max_size_test_H1))\n",
    "    \n",
    "    # train\n",
    "    barlength_train_H0 = [pds_train[i]['persist_0'][1] - pds_train[i]['persist_0'][0] for i in range(num_dgs_train)]\n",
    "    barlength_train_H1 = [pds_train[i]['persist_1'][1] - pds_train[i]['persist_1'][0] for i in range(num_dgs_train)]\n",
    "    # test\n",
    "    barlength_test_H0 = [pds_test[i]['persist_0'][1] - pds_test[i]['persist_0'][0] for i in range(num_dgs_test)]\n",
    "    barlength_test_H1 = [pds_test[i]['persist_1'][1] - pds_test[i]['persist_1'][0] for i in range(num_dgs_test)]\n",
    "\n",
    "    # TRAIN\n",
    "    for i in range(num_dgs_train):\n",
    "        train_vecs_H0[i, :barlength_train_H0[i].shape[0]] = np.sort(barlength_train_H0[i])[::-1]\n",
    "    for i in range(num_dgs_train):\n",
    "        train_vecs_H1[i, :barlength_train_H1[i].shape[0]] = np.sort(barlength_train_H1[i])[::-1]\n",
    "    # H0 TEST\n",
    "    for i in range(num_dgs_test):\n",
    "        test_vecs_H0[i, :barlength_test_H0[i].shape[0]] = np.sort(barlength_test_H0[i])[::-1]\n",
    "    for i in range(num_dgs_test):\n",
    "        test_vecs_H1[i, :barlength_test_H1[i].shape[0]] = np.sort(barlength_test_H1[i])[::-1]\n",
    "\n",
    "    train_vecs_H0 = train_vecs_H0[:,:preproc_size]*1000 ##* ALTERNATIVE for TEMPERATURE??\n",
    "    train_vecs_H1 = train_vecs_H1[:,:preproc_size]*100\n",
    "    test_vecs_H0 = test_vecs_H0[:,:preproc_size]*1000\n",
    "    test_vecs_H1 = test_vecs_H1[:,:preproc_size]*100\n",
    "\n",
    "    # mean and std in training set\n",
    "    train_mean = np.mean(train_vecs_H0, axis=0)\n",
    "    train_std = np.std(train_vecs_H0, axis=0)\n",
    "\n",
    "\n",
    "elif METHOD == 'Gauss_Transf':\n",
    "\n",
    "    emb_dim = 15\n",
    "    train_vecs_H0 = np.zeros((num_dgs_train, max_size_H0, emb_dim))\n",
    "    train_vecs_H1 = np.zeros((num_dgs_train, max_size_train_H1, emb_dim))\n",
    "    test_vecs_H0 = np.zeros((num_dgs_test, max_size_H0, emb_dim))\n",
    "    test_vecs_H1 = np.zeros((num_dgs_test, max_size_test_H1, emb_dim))\n",
    "\n",
    "    # about 15 minutes:\n",
    "    # train\n",
    "    barlength_train_H0 = [gaussian_transformation(np.array(pds_train[i]['persist_0']).transpose()) for i in range(num_dgs_train)] # shape (3500, 999, 15)\n",
    "    barlength_train_H1 = [gaussian_transformation(np.array(pds_train[i]['persist_1']).transpose()) for i in range(num_dgs_train)]\n",
    "    # test\n",
    "    barlength_test_H0 = [gaussian_transformation(np.array(pds_test[i]['persist_0']).transpose()) for i in range(num_dgs_test)]\n",
    "    barlength_test_H1 = [gaussian_transformation(np.array(pds_test[i]['persist_1']).transpose()) for i in range(num_dgs_test)]\n",
    "    \n",
    "    # # H0 TRAIN\n",
    "    for i in range(num_dgs_train):\n",
    "        train_vecs_H0[i, :barlength_train_H0[i].shape[0], :] = barlength_train_H0[i]\n",
    "    # # H1 TRAIN\n",
    "    for i in range(num_dgs_train):\n",
    "        train_vecs_H1[i, :barlength_train_H1[i].shape[0],:] = barlength_train_H1[i]\n",
    "    # # H0 TEST\n",
    "    for i in range(num_dgs_test):\n",
    "        test_vecs_H0[i, :barlength_test_H0[i].shape[0],:] = barlength_test_H0[i]\n",
    "    for i in range(num_dgs_test):\n",
    "        test_vecs_H1[i, :barlength_test_H1[i].shape[0],:] = barlength_test_H1[i]\n",
    "\n",
    "    # here I can't create a matrix, here we have a tensor\n",
    "    train_vecs_H0 = train_vecs_H0[:,:preproc_size,:]*1000 ##* ALTERNATIVE for TEMPERATURE??\n",
    "    train_vecs_H1 = train_vecs_H1[:,:preproc_size,:]*100\n",
    "    test_vecs_H0 = test_vecs_H0[:,:preproc_size,:]*1000\n",
    "    test_vecs_H1 = test_vecs_H1[:,:preproc_size,:]*100\n",
    "\n",
    "\n",
    "elif METHOD == 'Density':\n",
    "    from scipy.stats import norm\n",
    "    mu = 0.15\n",
    "    sigma = 0.05\n",
    "\n",
    "    train_vecs_H0 = np.zeros((num_dgs_train, max_size_H0))\n",
    "    train_vecs_H1 = np.zeros((num_dgs_train, max_size_train_H1))\n",
    "    test_vecs_H0 = np.zeros((num_dgs_test, max_size_H0))\n",
    "    test_vecs_H1 = np.zeros((num_dgs_test, max_size_test_H1))\n",
    "\n",
    "    barlength_train_H0 = [norm.cdf(np.array(pds_train[i]['persist_0'][1]), loc=mu, scale=sigma) - norm.cdf(np.array(pds_train[i]['persist_0'][0]), loc=mu, scale=sigma) for i in range(num_dgs_train)]\n",
    "    barlength_train_H1 = [norm.cdf(np.array(pds_train[i]['persist_1'][1]), loc=mu, scale=sigma) - norm.cdf(np.array(pds_train[i]['persist_1'][0]), loc=mu, scale=sigma) for i in range(num_dgs_train)]\n",
    "    barlength_test_H0 = [norm.cdf(np.array(pds_test[i]['persist_0'][1]), loc=mu, scale=sigma) - norm.cdf(np.array(pds_test[i]['persist_0'][0]), loc=mu, scale=sigma) for i in range(num_dgs_test)]\n",
    "    barlength_test_H1 = [norm.cdf(np.array(pds_test[i]['persist_1'][1]), loc=mu, scale=sigma) - norm.cdf(np.array(pds_test[i]['persist_1'][0]), loc=mu, scale=sigma) for i in range(num_dgs_test)]\n",
    "    \n",
    "    # TRAIN\n",
    "    for i in range(num_dgs_train):\n",
    "        train_vecs_H0[i, :barlength_train_H0[i].shape[0]] = barlength_train_H0[i][::-1]\n",
    "    for i in range(num_dgs_train):\n",
    "        train_vecs_H1[i, :barlength_train_H1[i].shape[0]] = barlength_train_H1[i][::-1]\n",
    "    # H0 TEST\n",
    "    for i in range(num_dgs_test):\n",
    "        test_vecs_H0[i, :barlength_test_H0[i].shape[0]] = barlength_test_H0[i][::-1]\n",
    "    for i in range(num_dgs_test):\n",
    "        test_vecs_H1[i, :barlength_test_H1[i].shape[0]] = barlength_test_H1[i][::-1]\n",
    "    \n",
    "    train_vecs_H0 = train_vecs_H0[:,:preproc_size]*1000 ##* ALTERNATIVE for TEMPERATURE??\n",
    "    train_vecs_H1 = train_vecs_H1[:,:preproc_size]*100\n",
    "    test_vecs_H0 = test_vecs_H0[:,:preproc_size]*1000\n",
    "    test_vecs_H1 = test_vecs_H1[:,:preproc_size]*100\n",
    "\n",
    "# mean and std in training set\n",
    "# I have a matrix with gaussian transformation\n",
    "train_mean = np.mean(train_vecs_H0, axis=0)\n",
    "train_std = np.std(train_vecs_H0, axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_mean, axis=0)\n",
    "train_std = np.std(train_std, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 750), (3500, 750), (1500, 750), (1500, 750))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_H0.shape, train_vecs_H1.shape, test_vecs_H0.shape, test_vecs_H1.shape\n",
    "# ((3500, 750, 15), (3500, 750, 15), (1500, 750, 15), (1500, 750, 15)) if METHOD == 'Gauss_Transf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500,), (1500,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes = np.array([pds_train[i]['id_class'] for i in range(num_dgs_train)])\n",
    "test_classes = np.array([pds_test[i]['id_class'] for i in range(num_dgs_test)])\n",
    "train_classes.shape, test_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 12)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batching train and test datasets with respective labels\n",
    "batch_size = 128\n",
    "\n",
    "if METHOD == 'Gauss_Transf':\n",
    "    n_train_batches = len(train_vecs_H0)//batch_size + 1\n",
    "    batched_train = []\n",
    "    for i in range(n_train_batches):\n",
    "        batched_train.append(\\\n",
    "            [torch.tensor(train_vecs_H0[i*batch_size:(i+1)*batch_size,:,:]),\\\n",
    "            torch.tensor(train_vecs_H1[i*batch_size:(i+1)*batch_size,:,:]),\\\n",
    "            torch.tensor(train_classes[i*batch_size:(i+1)*batch_size])])\n",
    "        \n",
    "    n_test_batches = len(test_vecs_H0)//batch_size + 1\n",
    "    batched_test = []\n",
    "    for i in range(n_test_batches):\n",
    "        batched_test.append(\\\n",
    "            [torch.tensor(test_vecs_H0[i*batch_size:(i+1)*batch_size,:,:]),\\\n",
    "            torch.tensor(test_vecs_H1[i*batch_size:(i+1)*batch_size,:,:]),\\\n",
    "            torch.tensor(test_classes[i*batch_size:(i+1)*batch_size])])\n",
    "\n",
    "else:\n",
    "    n_train_batches = len(train_vecs_H0)//batch_size + 1\n",
    "    batched_train = []\n",
    "    for i in range(n_train_batches):\n",
    "        batched_train.append(\\\n",
    "            [torch.tensor(train_vecs_H0[i*batch_size:(i+1)*batch_size,:]),\\\n",
    "            torch.tensor(train_vecs_H1[i*batch_size:(i+1)*batch_size,:]),\\\n",
    "            torch.tensor(train_classes[i*batch_size:(i+1)*batch_size])])\n",
    "        \n",
    "    n_test_batches = len(test_vecs_H0)//batch_size + 1\n",
    "    batched_test = []\n",
    "    for i in range(n_test_batches):\n",
    "        batched_test.append(\\\n",
    "            [torch.tensor(test_vecs_H0[i*batch_size:(i+1)*batch_size,:]),\\\n",
    "            torch.tensor(test_vecs_H1[i*batch_size:(i+1)*batch_size,:]),\\\n",
    "            torch.tensor(test_classes[i*batch_size:(i+1)*batch_size])])\n",
    "n_train_batches, n_test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERSISTENCE IMAGE $L_\\infty$ Network ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linfLayer(nn.Module):\n",
    "    def __init__(self, in_blocks, out_blocks, layer_norm_params=None):\n",
    "        super().__init__()\n",
    "        #todo: add mean and variance of the dataset\n",
    "\n",
    "        if layer_norm_params is not None:\n",
    "          mean = torch.tensor(layer_norm_params[0])\n",
    "          std = torch.tensor(layer_norm_params[1])\n",
    "          params = mean + torch.randn((out_blocks, in_blocks)) * std # normalization according to training values\n",
    "          self.params = torch.nn.Parameter(data=params)\n",
    "        else:\n",
    "          params = torch.randn((out_blocks, in_blocks))\n",
    "          self.params = torch.nn.Parameter(data=params)\n",
    "\n",
    "    def forward(self, x, p):\n",
    "        res = torch.cdist(x, self.params, p=p) #torch.cdist but with custom params\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "inf=float(\"inf\")\n",
    "\n",
    "# p_schedule = np.ones(epochs)*inf\n",
    "e1, e2 = 20,40 # e1+e2 < epochs !!\n",
    "p_schedule = np.ones(epochs)*8\n",
    "p_schedule[e1:e1+e2] = np.linspace(8, 100, e2)\n",
    "p_schedule[e1+e2:] = inf\n",
    "\n",
    "save_folder = f'./LinfModels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linf_PersImage(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_size, topK, mean, std) -> None:\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.topK = topK\n",
    "\n",
    "        self.layer1_H0 = linfLayer(in_blocks=emb_dim, out_blocks=150, layer_norm_params=[train_mean, train_std])\n",
    "        self.layer2_H0 = linfLayer(in_blocks=150, out_blocks=80)\n",
    "        # self.layer3_H0 = linfLayer(in_blocks=30, out_blocks=20)\n",
    "        self.layer4_H0 = linfLayer(in_blocks=80, out_blocks=hidden_size)\n",
    "        \n",
    "        self.layer1_H1 = linfLayer(in_blocks=emb_dim, out_blocks=150, layer_norm_params=[train_mean, train_std])\n",
    "        self.layer2_H1 = linfLayer(in_blocks=150, out_blocks=80)\n",
    "        # self.layer3_H1 = linfLayer(in_blocks=30, out_blocks=20)\n",
    "        self.layer4_H1 = linfLayer(in_blocks=80, out_blocks=hidden_size)\n",
    "\n",
    "        # self.layer1_concat = linfLayer(topK*(hidden_size*2), 80)\n",
    "        self.layer1_concat = linfLayer(topK*2, 80)\n",
    "        self.layer2_concat = linfLayer(80, 20)\n",
    "        self.layer3_concat = linfLayer(20, 5)\n",
    "\n",
    "    def forward(self, h0, h1, p):\n",
    "        B = h0.shape[0]\n",
    "        h0 = self.layer1_H0(h0, p)\n",
    "        h0 = h0 - h0.mean(axis=0)\n",
    "        h0 = self.layer2_H0(h0, p)\n",
    "        h0 = h0 - h0.mean(axis=0)\n",
    "        # h0 = self.layer3_H0(h0, p)\n",
    "        # h0 = h0 - h0.mean(axis=0)\n",
    "        h0 = self.layer4_H0(h0, p)\n",
    "        h0 = torch.topk(h0, self.topK, dim=1)[0].view(B,-1)\n",
    "        \n",
    "        h1 = self.layer1_H1(h1, p)\n",
    "        h1 = h1 - h1.mean(axis=0)\n",
    "        h1 = self.layer2_H1(h1, p)\n",
    "        h1 = h1 - h1.mean(axis=0)\n",
    "        # h1 = self.layer3_H1(h1, p)\n",
    "        # h1 = h1 - h1.mean(axis=0)\n",
    "        h1 = self.layer4_H1(h1, p)\n",
    "        h1 = torch.topk(h1, self.topK, dim=1)[0].view(B,-1)\n",
    "\n",
    "        concat = torch.cat((h0,h1), dim = 1)\n",
    "        # print(concat.shape)\n",
    "        concat = self.layer1_concat(concat, p)\n",
    "        concat = concat - concat.mean(axis=0)\n",
    "        concat = self.layer2_concat(concat, p)\n",
    "        concat = concat - concat.mean(axis=0)\n",
    "        output = self.layer3_concat(concat, p)\n",
    "\n",
    "        return -output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, if_plot):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    target_labs = np.array([])\n",
    "    pred_labs = np.array([])\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # for batch in tqdm(test_data):\n",
    "        for (h0, h1, cl) in test_data:\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(h0, h1, inf)\n",
    "            # Get predicted labels\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # Total number of labels\n",
    "            total += cl.size(0)\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == cl).sum().item()\n",
    "            target_labs = np.append(target_labs, cl)\n",
    "            pred_labs = np.append(pred_labs, predicted)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    # print('Accuracy on the test set: {:.2f}%'.format(accuracy))\n",
    "\n",
    "    if if_plot:\n",
    "        cm = confusion_matrix(np.array(target_labs), np.array(pred_labs))\n",
    "        classes = [1,2,3,4,5]\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_418/2226993550.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = model(torch.tensor(h0), torch.tensor(h1), p_schedule[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100 --> IMPROVEMENT from 0.000 to 52.333 \t\n",
      "Epoch   2/100 --> IMPROVEMENT from 52.333 to 57.267 \t\n",
      "Epoch   6/100 --> IMPROVEMENT from 57.267 to 59.000 \t\n",
      "Epoch  11/100 --> IMPROVEMENT from 59.000 to 59.133 \t\n",
      " Epoch  23\tloss : 1.23395, p: inf"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     23\u001b[0m     tot_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss)\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(tot_loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Linf_PersImage(preproc_size, 15, 10, train_mean, train_std)\n",
    "# Model , Optimizer, Loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.05, eps=1e-06)\n",
    "# list(model.parameters())[0].shape\n",
    "losses = []\n",
    "best_acc = 0.0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for (h0, h1, cl) in batched_train:\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        \n",
    "        #calculate output\n",
    "        outputs = model(torch.tensor(h0), torch.tensor(h1), p_schedule[i])\n",
    "\n",
    "        #calculate loss\n",
    "        loss = loss_fn(outputs, cl.long())\n",
    "\n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        tot_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    losses.append(tot_loss)\n",
    "    test_acc = test_model(model, batched_test, if_plot=False)\n",
    "    if best_acc < test_acc:\n",
    "        if test_acc > 80.0:\n",
    "            torch.save(model.state_dict(), save_folder + f'PURE_{preproc_size}_acc_{test_acc:.3f}.pth')\n",
    "        print(f'\\rEpoch {i+1:3}/{epochs} --> IMPROVEMENT from {best_acc:.3f} to {test_acc:.3f} \\t')\n",
    "        best_acc = test_acc\n",
    "    else:\n",
    "        # print(f'> Test Accuracy = {test_acc} [best = {best_acc}]')\n",
    "        print(f\"\\r Epoch {i+1:3}\\tloss : {loss:.5f}, p: {p_schedule[i]}\", end='')\n",
    "\n",
    "plt.plot(np.array(losses))\n",
    "train_acc = test_model(model, batched_train, if_plot=True)\n",
    "test_acc = test_model(model, batched_test, if_plot=True)\n",
    "train_acc, test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advtopo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
