{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING NNs TO CLASSIFY POINT CLOUDS STARTING FROM THEIR PERSISTENT DIAGRAM:\n",
    "\n",
    "> Generate Train and Test Dataset with certain amounts of point clouds\n",
    "\n",
    "    > Actual dataset\n",
    "\n",
    "    > PDs from each sample\n",
    "\n",
    "> Crete NNs\n",
    "\n",
    "    > DEEPESET\n",
    "\n",
    "    > PERSLAY\n",
    "\n",
    "> Check accuracy of Models, on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import functools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import gudhi as gd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def rand(shape, low, high):\n",
    "    \"\"\"Tensor of random numbers, uniformly distributed on [low, high].\"\"\"\n",
    "    return torch.rand(shape) * (high - low) + low\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Return the number of trainable parameters of a model (the total number of scalars).\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I WANT TO COMPARE POINT WITH 1K POINTS WITH THE SAME STARTING POINT\n",
    "def generate_orbits(n_points_per_orbit = 1000, params = [2.5, 3.5, 4.0, 4.1, 4.3], same_init_point = True):\n",
    "    # create point clouds \n",
    "    ORBITS = np.zeros([len(params), n_points_per_orbit, 2])\n",
    "    xcur_0, ycur_0 = np.random.rand(), np.random.rand() # not necesary to save the first one\n",
    "    for id_pc, param in enumerate(params): # id_point_cloud\n",
    "        if same_init_point:\n",
    "            xcur, ycur = xcur_0, ycur_0 # not necesary to save the first one\n",
    "        else:\n",
    "            xcur, ycur =np.random.rand(), np.random.rand()\n",
    "        for id_pt in range(n_points_per_orbit): # id_point\n",
    "            xcur = (xcur + param * ycur * (1. - ycur)) % 1\n",
    "            ycur = (ycur + param * xcur * (1. - xcur)) % 1\n",
    "            ORBITS[id_pc, id_pt, :] = [xcur, ycur]\n",
    "    return ORBITS\n",
    "\n",
    "# function from [len(params), n_points, 2] to alpha-complex and persistence diagram\n",
    "# to create PDs we need to: points -> skeleton(ac) -> simplex(st) -> persistence(pers)\n",
    "# for each element of the dataset we'll have len(params) PDs to be compared\n",
    "import gudhi as gd\n",
    "def extract_PD(cloud, id_class):\n",
    "    \"\"\"extract a dict \n",
    "\n",
    "    Args:\n",
    "        cloud (_type_): array [1000,2] composing th ewhole point cloud\n",
    "        id_class (_type_): index about the class of membership\n",
    "\n",
    "    Returns:\n",
    "        dict: with keys ['persist_0','persist_1','id_class']\n",
    "    \"\"\"\n",
    "    # for every point cloud we create a dictionary storing the label and its persistence\n",
    "    # usage of dictionary to store each other possible data linked to the point clous\n",
    "    ac = gd.AlphaComplex(points=cloud)\n",
    "    st = ac.create_simplex_tree()\n",
    "    pers = st.persistence()\n",
    "    #? EXTENDED PERSISTENCE?\n",
    "    #! TRANSPOSE TO HAVE THEN [BATCH SIZE, 2, NUM POINTS]\n",
    "    pers_0 = np.array(st.persistence_intervals_in_dimension(0)).transpose()#*100\n",
    "    pers_1 = np.array(st.persistence_intervals_in_dimension(1)).transpose()#*100\n",
    "    pers_dict = {\n",
    "        'cloud': cloud, #* UNCOMMENT THE LINE IF YOU WANT TO VISUALIZE POINT CLOUDS LATER\n",
    "        # 'skeleton': ac, # no more used\n",
    "        # 'complex': st, # used for bottleneck distance\n",
    "        'persist_0': pers_0[:,:-1], # removing the last barcode, the one with inf\n",
    "        'persist_1': pers_1, # here we should never have inf, since [0,1]^2 is compact/bounded  \n",
    "        # 'persist': pers, # actual PD\n",
    "        'id_class': id_class # label for classification\n",
    "    }\n",
    "    return pers_dict\n",
    "\n",
    "# def pd_to_tensor(pd, value):\n",
    "#     # tens_intervals = torch.tensor([list(tp[1]) for tp in pd ])\n",
    "#     tens_intervals = torch.tensor(pd).float()\n",
    "#     # this tensor can contain float('inf)\n",
    "#     tens_intervals[torch.isinf(tens_intervals)] = value\n",
    "#     return tens_intervals\n",
    "\n",
    "def preproc_prom(tens, prom):\n",
    "    diff = tens[1] - tens[0]\n",
    "    sort_diff = torch.argsort(diff, descending=True)\n",
    "    sort_tens = tens[:,sort_diff]\n",
    "    return sort_tens[:,:prom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create TRAIN Point Clouds: 100%|██████████| 700/700 [00:35<00:00, 19.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_batched_data) = 28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create TEST Point Clouds: 100%|██████████| 300/300 [00:18<00:00, 16.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_batched_data) = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### FULL DATA GENERATION \n",
    "# (~2 mins) ---------------------------------------\n",
    "\n",
    "# hyper params\n",
    "n_points = 1000\n",
    "params = [2.5, 3.5, 4.0, 4.1, 4.3]\n",
    "same_init_point = True\n",
    "n_seq_per_dataset = [700, 300] # I want [i, len(params), n_points, 2]\n",
    "\n",
    "batch_size = 128\n",
    "extended_pers = False\n",
    "k_pd_preproc = 500\n",
    "\n",
    "# init list fo persistence diagrams\n",
    "pds_train = []\n",
    "pds_test = []\n",
    "\n",
    "# TRAIN \n",
    "for i in tqdm(range(n_seq_per_dataset[0]), desc='Create TRAIN Point Clouds'):\n",
    "    ORBS = generate_orbits(n_points, params, same_init_point) # CREATE THE 5 POINT CLOUDS\n",
    "    for j in range(ORBS.shape[0]):\n",
    "        ij_pers = extract_PD(ORBS[j,:,:], j) # EXTRACT PDs\n",
    "        pds_train.append(ij_pers) # STORE IN THE LIST pds_train\n",
    "\n",
    "train_batched_data = [] # BATCHING DATA FOR THE NN\n",
    "batching = len(pds_train)//batch_size\n",
    "for i in range(batching):\n",
    "    train_batched_data.append(pds_train[i*batch_size:(i+1)*batch_size])\n",
    "# check if we have to add the last batch\n",
    "if batching*batch_size != len(pds_train):\n",
    "    train_batched_data.append(pds_train[batching*batch_size:])\n",
    "print(f'{len(train_batched_data) = }\\n')\n",
    "    \n",
    "\n",
    "# TEST\n",
    "for i in tqdm(range(n_seq_per_dataset[1]), desc='Create TEST Point Clouds'):\n",
    "    ORBS = generate_orbits(n_points, params, same_init_point) # CREATE THE 5 POINT CLOUDS\n",
    "    for j in range(ORBS.shape[0]):\n",
    "        ij_pers = extract_PD(ORBS[j,:,:], j) # EXTRACT PDs\n",
    "        pds_test.append(ij_pers) # STORE IN THE LIST pds_test\n",
    "\n",
    "test_batched_data = [] # BATCHING DATA FOR THE NN\n",
    "batching = len(pds_test)//batch_size\n",
    "for i in range(batching):\n",
    "    test_batched_data.append(pds_test[i*batch_size:(i+1)*batch_size])\n",
    "# check if we have to add the last batch\n",
    "if batching*batch_size != len(pds_test):\n",
    "    test_batched_data.append(pds_test[batching*batch_size:])\n",
    "print(f'{len(test_batched_data) = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{False}\n",
      "{False}\n",
      "{False}\n",
      "{False}\n"
     ]
    }
   ],
   "source": [
    "# check the order of persistences; BRUTE FORCE: expecting 4 {False}\n",
    "contains_inf = [ np.any([np.any(np.isinf(lst)) for lst in l['persist_0']]) for l in pds_train ]; print(set(contains_inf))\n",
    "contains_inf = [ np.any([np.any(np.isinf(lst)) for lst in l['persist_1']]) for l in pds_train ]; print(set(contains_inf))\n",
    "contains_inf = [ np.any([np.any(np.isinf(lst)) for lst in l['persist_0']]) for l in pds_test ]; print(set(contains_inf))\n",
    "contains_inf = [ np.any([np.any(np.isinf(lst)) for lst in l['persist_1']]) for l in pds_test ]; print(set(contains_inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f225462a500>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dims = [sample['persist_1'].shape[1] for sample in pds_train]\n",
    "plt.plot(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level =  1   => num of barcodes = 35\n",
      "level = 100  => num of barcodes = 69\n",
      "level = 200  => num of barcodes = 77\n",
      "level = 300  => num of barcodes = 82\n",
      "level = 400  => num of barcodes = 86\n",
      "level = 500  => num of barcodes = 91\n",
      "level = 600  => num of barcodes = 93\n",
      "level = 700  => num of barcodes = 98\n",
      "level = 800  => num of barcodes = 105\n",
      "level = 900  => num of barcodes = 1082\n",
      "level = 1000 => num of barcodes = 3490\n",
      "level = 1025 => num of barcodes = 3500\n"
     ]
    }
   ],
   "source": [
    "level_to_check = [1,100,200,300,400,500,600,700,800,900,1000,1025]\n",
    "for level in level_to_check:\n",
    "    print(f'level = {level:^4} => num of barcodes = {len([x for x in dims if x <= level])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_1 = [idx for idx, dim in enumerate(dims) if dim<100]\n",
    "cl_dim_1 = []\n",
    "for dim in dim_1:\n",
    "    point_cloud = pds_train[dim]['cloud']\n",
    "    class_id = pds_train[dim]['id_class']\n",
    "    # plt.cla()\n",
    "    # plt.scatter(point_cloud[:,0], point_cloud[:,1])\n",
    "    # print(f'\\n - {dim} --- {class_id = } -----------')\n",
    "    # plt.show()\n",
    "    if class_id not in cl_dim_1:\n",
    "        cl_dim_1.append(class_id)\n",
    "cl_dim_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [2.5, 3.5, 4.0, 4.1, 4.3]\n",
    "n_points = 1000\n",
    "same_init_point = True\n",
    "n_seq_per_dataset = [700, 700] # I want [i, len(params), n_points, 2]\n",
    "\n",
    "train_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "for i in tqdm(range(n_seq_per_dataset[0]), desc='Create Train Point Clouds'):\n",
    "    ORB = generate_orbits(n_points, params, same_init_point)\n",
    "    train_dataset.append(ORB)\n",
    "train_dataset = np.stack(train_dataset)\n",
    "\n",
    "for i in tqdm(range(n_seq_per_dataset[1])):\n",
    "    ORB = generate_orbits(n_points, params, same_init_point)\n",
    "    test_dataset.append(ORB)\n",
    "test_dataset = np.stack(test_dataset)\n",
    "\n",
    "train_dataset.shape, test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [2.5, 3.5, 4.0, 4.1, 4.3]\n",
    "n_points = 1000\n",
    "same_init_point = True\n",
    "n_seq_per_dataset = [700, 700] # I want [i, len(params), n_points, 2]\n",
    "\n",
    "train_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "for i in tqdm(range(n_seq_per_dataset[0]), desc='Create Train Point Clouds'):\n",
    "    ORB = generate_orbits(n_points, params, same_init_point)\n",
    "    train_dataset.append(ORB)\n",
    "train_dataset = np.stack(train_dataset)\n",
    "\n",
    "for i in tqdm(range(n_seq_per_dataset[1])):\n",
    "    ORB = generate_orbits(n_points, params, same_init_point)\n",
    "    test_dataset.append(ORB)\n",
    "test_dataset = np.stack(test_dataset)\n",
    "\n",
    "train_dataset.shape, test_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute PDs for each point cloud, associating it wrt its class' label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds_train = []\n",
    "for i in tqdm(range(train_dataset.shape[0])):\n",
    "    for j in range(train_dataset.shape[1]):\n",
    "        ij_pers = extract_PD(train_dataset[i,j,:,:], j)\n",
    "        pds_train.append(ij_pers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds_test = []\n",
    "for i in tqdm(range(test_dataset.shape[0])):\n",
    "    for j in range(test_dataset.shape[1]):\n",
    "        ij_pers = extract_PD(test_dataset[i,j,:,:], j)\n",
    "        pds_test.append(ij_pers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pds_train), len(pds_test) # check dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gudhi as gd\n",
    "im_res = 40\n",
    "PI = gd.representations.PersistenceImage(bandwidth=1e-4, weight=lambda x: x[1]**2, \\\n",
    "                                         im_range=[0,.004,0,.004], resolution=[im_res,im_res])\n",
    "                                         \n",
    "pd_0 = pds_train[1][0]\n",
    "pd_0_compl = pd_0['complex']\n",
    "pi = PI.fit_transform([pd_0_compl.persistence_intervals_in_dimension(1)])\n",
    "plt.imshow(np.flip(np.reshape(pi[0], [im_res,im_res]), 0))\n",
    "plt.title(\"Persistence Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying DEEPSET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE THERE IS A PROBLEM: HOW DO WE WANT TO HANDLE THE CHANGING SIZE OF H1?!\n",
    "\n",
    "A possible solution is transform it in a Pers Image, but we leave the 'only tda' path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs: DEEPSET & PARSLAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PermutationEquivariantLayer(nn.Module):\n",
    "    \"PERSLAY LAYER\"\n",
    "    def __init__(self, dimension, perm_op:str, lbda, b, gamma):\n",
    "        super(PermutationEquivariantLayer, self).__init__()\n",
    "        self.dimension = dimension\n",
    "        self.perm_op = perm_op\n",
    "        self.lbda = lbda\n",
    "        self.b = b\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inp):\n",
    "        dimension_before, num_pts = inp.shape[2], inp.shape[1]\n",
    "        b = self.b.unsqueeze(0).unsqueeze(0)\n",
    "        A = torch.einsum(\"ijk,kl->ijl\", inp, self.lbda).reshape(-1, num_pts, self.dimension)\n",
    "        if self.perm_op is not None:\n",
    "            if self.perm_op == \"max\":\n",
    "                beta = torch.tile(torch.unsqueeze(torch.max(inp, dim=1)[0], 1), [1, num_pts, 1])\n",
    "            elif self.perm_op == \"min\":\n",
    "                beta = torch.tile(torch.unsqueeze(torch.min(inp, dim=1)[0], 1), [1, num_pts, 1])\n",
    "            elif self.perm_op == \"sum\":\n",
    "                beta = torch.tile(torch.unsqueeze(torch.sum(inp, dim=1), 1), [1, num_pts, 1])\n",
    "            else:\n",
    "                raise Exception(\"perm_op should be min, max or sum\")\n",
    "            B = torch.einsum(\"ijk,kl->ijl\", beta, self.gamma).reshape(-1, num_pts, self.dimension)\n",
    "            return A - B + b\n",
    "        else:\n",
    "            return A + b\n",
    "\n",
    "class RationalHatLayer(nn.Module):\n",
    "    \"RATIONAL HAT LAYER\"\n",
    "    def __init__(self, q, mu, r):\n",
    "        super(RationalHatLayer, self).__init__()\n",
    "        self.q = q\n",
    "        self.mu = mu\n",
    "        self.r = r\n",
    "\n",
    "    def forward(self, inp):\n",
    "        mu, r = self.mu.unsqueeze(0).unsqueeze(0), self.r.unsqueeze(0).unsqueeze(0)\n",
    "        dimension_before, num_pts = inp.shape[2], inp.shape[1]\n",
    "        bc_inp = inp.unsqueeze(-1)\n",
    "        norms = torch.norm(bc_inp - mu, p=self.q, dim=2)\n",
    "        return 1 / (1 + norms) - 1 / (1 + torch.abs(torch.abs(r) - norms))\n",
    "\n",
    "class RationalLayer(nn.Module):\n",
    "    def __init__(self, mu, sg, al):\n",
    "        super(RationalLayer, self).__init__()\n",
    "        self.mu = mu\n",
    "        self.sg = sg\n",
    "        self.al = al\n",
    "\n",
    "    def forward(self, inp):\n",
    "        mu, sg, al = self.mu.unsqueeze(0).unsqueeze(0), self.sg.unsqueeze(0).unsqueeze(0), self.al.unsqueeze(0).unsqueeze(0)\n",
    "        dimension_before, num_pts = inp.shape[2], inp.shape[1]\n",
    "        bc_inp = inp.unsqueeze(-1)\n",
    "        return 1 / torch.pow(1 + torch.sum(torch.abs(bc_inp - mu) * torch.abs(sg), dim=2), al)\n",
    "\n",
    "class ExponentialLayer(nn.Module):\n",
    "    def __init__(self, mu, sg):\n",
    "        super(ExponentialLayer, self).__init__()\n",
    "        self.mu = mu\n",
    "        self.sg = sg\n",
    "\n",
    "    def forward(self, inp):\n",
    "        mu, sg = self.mu.unsqueeze(0).unsqueeze(0), self.sg.unsqueeze(0).unsqueeze(0)\n",
    "        dimension_before, num_pts = inp.shape[2], inp.shape[1]\n",
    "        bc_inp = inp.unsqueeze(-1)\n",
    "        return torch.exp(torch.sum(-torch.square(bc_inp - mu) * torch.square(sg), dim=2))\n",
    "\n",
    "class LandscapeLayer(nn.Module):\n",
    "    def __init__(self, sp):\n",
    "        super(LandscapeLayer, self).__init__()\n",
    "        self.sp = sp\n",
    "\n",
    "    def forward(self, inp):\n",
    "        sp = self.sp.unsqueeze(0).unsqueeze(0)\n",
    "        return torch.maximum(.5 * (inp[:, :, 1:2] - inp[:, :, 0:1]) - torch.abs(sp - .5 * (inp[:, :, 1:2] + inp[:, :, 0:1])), torch.tensor([0.]))\n",
    "\n",
    "\n",
    "class BettiLayer(nn.Module):\n",
    "    def __init__(self, theta, sp):\n",
    "        super(BettiLayer, self).__init__()\n",
    "        self.theta = theta\n",
    "        self.sp = sp\n",
    "\n",
    "    def forward(self, inp):\n",
    "        sp = self.sp.unsqueeze(0).unsqueeze(0)\n",
    "        X, Y = inp[:, :, 0:1], inp[:, :, 1:2]\n",
    "        return 1. / (1. + torch.exp(-self.theta * (.5 * (Y - X) - torch.abs(sp - .5 * (Y + X)))))\n",
    "\n",
    "\n",
    "class EntropyLayer(nn.Module):\n",
    "    def __init__(self, theta, sp):\n",
    "        super(EntropyLayer, self).__init__()\n",
    "        self.theta = theta\n",
    "        self.sp = sp\n",
    "\n",
    "    def forward(self, inp):\n",
    "        sp = self.sp.unsqueeze(0).unsqueeze(0)\n",
    "        bp_inp = torch.einsum(\"ijk,kl->ijl\", inp, torch.tensor([[1., -1.], [0., 1.]]))\n",
    "        L, X, Y = bp_inp[:, :, 1:2], bp_inp[:, :, 0:1], bp_inp[:, :, 0:1] + bp_inp[:, :, 1:2]\n",
    "        LN = L * (1. / torch.unsqueeze(torch.matmul(L[:, :, 0], torch.ones([L.shape[1], 1])), -1))\n",
    "        entropy_terms = torch.where(LN > 0., -LN * torch.log(LN), LN)\n",
    "        return entropy_terms * (1. / (1. + torch.exp(-self.theta * (.5 * (Y - X) - torch.abs(sp - .5 * (Y + X))))))\n",
    "\n",
    "\n",
    "class ImageLayer(nn.Module):\n",
    "    def __init__(self, image_size, image_bnds, sg):\n",
    "        super(ImageLayer, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.image_bnds = image_bnds\n",
    "        self.sg = sg\n",
    "\n",
    "    def forward(self, inp):\n",
    "        bp_inp = torch.einsum(\"ijk,kl->ijl\", inp, torch.tensor([[1., -1.], [0., 1.]]))\n",
    "        dimension_before, num_pts = inp.shape[2], inp.shape[1]\n",
    "        coords = [torch.arange(start=self.image_bnds[i][0], end=self.image_bnds[i][1], step=(self.image_bnds[i][1] - self.image_bnds[i][0]) / self.image_size[i]) for i in range(dimension_before)]\n",
    "        M = torch.meshgrid(*coords)\n",
    "        mu = torch.cat([torch.unsqueeze(tens, 0) for tens in M], dim=0)\n",
    "        bc_inp = bp_inp.view(-1, num_pts, dimension_before, 1)\n",
    "        return torch.unsqueeze(torch.exp(torch.sum(-torch.square(bc_inp - mu) / (2 * torch.square(self.sg)), dim=2)) / (2 * np.pi * torch.square(self.sg)), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DEEPSET NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 7])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeepSetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepSetLayer(in_blocks, out_blocks) takes shape (batch, in_blocks, n) to (batch, out_blocks, n).\n",
    "    Each block of n scalars is treated as the S_n permutation representation, and maps between blocks are\n",
    "    S_n-equivariant.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_blocks, out_blocks):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_blocks = in_blocks\n",
    "        self.out_blocks = out_blocks\n",
    "        \n",
    "        # Initialisation tactic copied from nn.Linear in PyTorch\n",
    "        lim = (in_blocks)**-0.5 / 2\n",
    "\n",
    "        # Alpha corresponds to the identity, beta to the all-ones matrix, and gamma to the additive bias.\n",
    "        self.alpha = torch.nn.Parameter(data=rand((out_blocks, in_blocks), -lim, lim))\n",
    "        self.beta = torch.nn.Parameter(data=rand((out_blocks, in_blocks), -lim, lim))\n",
    "        self.gamma = torch.nn.Parameter(data=rand((out_blocks), -lim, lim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch, in_blocks, n)\n",
    "        return (\n",
    "            torch.einsum('...jz, ij -> ...iz', x, self.alpha)\n",
    "            + torch.einsum('...jz, ij -> ...iz', x.max(axis=-1)[0][..., None], self.beta)\n",
    "            + self.gamma[..., None]\n",
    "        )\n",
    "\n",
    "# Test if we got our tensor nonsense right. For a batch size of 6, and n = 7,\n",
    "# a DeepSetLayer(2, 3) should map shape (6, 2, 7) to (6, 3, 7).\n",
    "DeepSetLayer(2, 3)(torch.rand((6, 2, 7))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> EXTRACT AN INVARIANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0991,  0.0681, -0.9168, -0.2258,  0.4504],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeepSetSum(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepSetSum(blocks) takes a deep set layer of shape (batch, blocks, n) to a regular layer\n",
    "    of shape (batch, blocks) by projecting to the trivial representation and then extracting\n",
    "    a coordinate, eg\n",
    "        (1, 2, 3, 4) => project to trivial => (2.5, 2.5, 2.5, 2.5) => extract component => 2.5\n",
    "    \"\"\"\n",
    "    def __init__(self, blocks):\n",
    "        super().__init__()\n",
    "        \n",
    "        lim = (blocks)**-0.5 / 2\n",
    "        self.weight = torch.nn.Parameter(data=rand(blocks, -lim, lim))\n",
    "        self.bias = torch.nn.Parameter(data=rand(blocks, -lim, lim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.sum(dim=-1) * self.weight + self.bias\n",
    "  \n",
    "# Check shapes. A DeepSetSum(3) should take shape (batch, 3, n) to (batch, 3)\n",
    "DeepSetSum(5)(torch.rand((6, 5, 7)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0502, 0.3259],\n",
       "        [0.0918, 0.3669],\n",
       "        [0.0559, 0.2800],\n",
       "        [0.2241, 0.4390],\n",
       "        [0.0758, 0.2743],\n",
       "        [0.0411, 0.2047],\n",
       "        [0.0480, 0.2080],\n",
       "        [0.0727, 0.2318],\n",
       "        [0.0545, 0.2057],\n",
       "        [0.0376, 0.1887],\n",
       "        [0.0401, 0.1900],\n",
       "        [0.0489, 0.1919],\n",
       "        [0.0546, 0.1948],\n",
       "        [0.1101, 0.2470],\n",
       "        [0.2591, 0.3868],\n",
       "        [0.0392, 0.1669],\n",
       "        [0.0464, 0.1722],\n",
       "        [0.0501, 0.1729],\n",
       "        [0.0435, 0.1624],\n",
       "        [0.0398, 0.1577],\n",
       "        [0.0474, 0.1645],\n",
       "        [0.0444, 0.1601],\n",
       "        [0.0649, 0.1788],\n",
       "        [0.0549, 0.1647],\n",
       "        [0.0560, 0.1639]], dtype=torch.float64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = torch.tensor(batch_in_pd1[0]*1).T\n",
    "tops, top_idxs = torch.topk(l[:,1]-l[:,0],25,dim=0)\n",
    "l[top_idxs,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEPSET NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DEEP SET NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model structure. Note that we don't specify the input size anywhere!!!\n",
    "class Perslay_KTH(nn.Module):\n",
    "    def __init__(self, hidden_size:int = 10, alpha_0:bool = True, alpha_1:bool = True, prom:int = 500, top_k:int = 5, using_len_p1:bool = False):\n",
    "        super().__init__()\n",
    "        self.a0 = alpha_0\n",
    "        self.a1 = alpha_1\n",
    "        self.prom = prom\n",
    "        self.top_k = top_k\n",
    "        self.num_classes = 5\n",
    "        self.using_len_p1 = using_len_p1\n",
    "\n",
    "        self.ds_0_a = DeepSetLayer(2,25)\n",
    "        self.relu_0 = torch.nn.ReLU()\n",
    "        self.ds_0_b = DeepSetLayer(25,hidden_size)\n",
    "        # self.ds_0_c = DeepSetLayer(10,5)\n",
    "\n",
    "        self.ds_1_a = DeepSetLayer(2,25)\n",
    "        self.relu_1 = torch.nn.ReLU()\n",
    "        self.ds_1_b = DeepSetLayer(25,hidden_size)\n",
    "        # self.ds_1_c = DeepSetLayer(10,5)\n",
    "\n",
    "        if using_len_p1:\n",
    "            self.linear_dim_H1 = nn.Linear(1,self.top_k)\n",
    "            self.linear_labels = nn.Linear(self.top_k*(hidden_size*2+1), self.num_classes)\n",
    "        else:\n",
    "            self.linear_labels = nn.Linear(self.top_k*(hidden_size*2), self.num_classes)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, batch_pers_0, batch_pers_1):\n",
    "        labels = False\n",
    "        for p0, p1 in zip(batch_pers_0, batch_pers_1):# one PD at the time due to different cardinality betweeen different H1 barcodes\n",
    "            # I want to select the self.prom longest barcodes\n",
    "            p0 = torch.tensor(p0)*100 #? rescsaling\n",
    "            p0 = preproc_prom(p0, self.prom)\n",
    "            p0 = self.ds_0_a(p0.float())\n",
    "            p0 = self.ds_0_b(p0)\n",
    "            p0, _ = torch.topk(p0, self.top_k, dim=1)\n",
    "\n",
    "            # same for p1\n",
    "            p1 = torch.tensor(p1)*100 # not always with same len of p0\n",
    "\n",
    "            p1_shape = p1.shape[1] # number of elemnts in H1 persistence\n",
    "            if p1_shape<self.top_k:\n",
    "                dim_to_add = self.top_k - p1_shape\n",
    "                aux_zeros = torch.zeros(p1.size(0), dim_to_add)\n",
    "                p1 = torch.cat((p1, aux_zeros), dim=1)\n",
    "\n",
    "\n",
    "            p1 = preproc_prom(p1, self.prom)\n",
    "            p1 = self.ds_1_a(p1.float())\n",
    "            p1 = self.ds_1_b(p1)\n",
    "            try:\n",
    "                p1, _ = torch.topk(p1, self.top_k, dim=1)\n",
    "            except RuntimeError:\n",
    "                raise ValueError('')\n",
    "            \n",
    "            if self.using_len_p1 == True:\n",
    "                emb_len = self.linear_dim_H1(torch.tensor([p1_shape]).float())\n",
    "                concat = torch.cat((p0.view(-1), p1.view(-1), emb_len.view(-1)))\n",
    "            else:\n",
    "                concat = torch.cat((p0.view(-1), p1.view(-1)))\n",
    "\n",
    "            labs = self.linear_labels(concat).unsqueeze(0)\n",
    "\n",
    "            if isinstance(labels,bool):\n",
    "                labels = labs\n",
    "            else:\n",
    "                labels = torch.cat((labels, labs), dim = 0)\n",
    "                \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, if_plot):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    target_labs = np.array([])\n",
    "    pred_labs = np.array([])\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # for batch in tqdm(test_data):\n",
    "        for batch in test_data:\n",
    "            batch_in_pd0 = [sample['persist_0'] for sample in batch] # get tersor of persistence\n",
    "            batch_in_pd1 = [sample['persist_1'] for sample in batch] # get tersor of persistence\n",
    "            batch_target = torch.tensor([sample['id_class'] for sample in batch]) # get target labels\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_in_pd0, batch_in_pd1)\n",
    "            # Get predicted labels\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # Total number of labels\n",
    "            total += batch_target.size(0)\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == batch_target).sum().item()\n",
    "            target_labs = np.append(target_labs, predicted)\n",
    "            pred_labs = np.append(pred_labs, batch_target)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    # print('Accuracy on the test set: {:.2f}%'.format(accuracy))\n",
    "\n",
    "    if if_plot:\n",
    "        cm = confusion_matrix(np.array(target_labs), np.array(pred_labs))\n",
    "        classes = [1,2,3,4,5]\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('TEST Confusion Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, loss 27.963971495628357\n",
      "--> IMPROVEMENT from 0.0 to 73.53333333333333\n",
      "Epoch 2/300, loss 21.609690368175507\n",
      "> Test Accuracy = 72.53333333333333 [best = 73.53333333333333]\n",
      "Epoch 3/300, loss 19.85062962770462\n",
      "--> IMPROVEMENT from 73.53333333333333 to 76.13333333333334\n",
      "Epoch 4/300, loss 19.1639501452446\n",
      "--> IMPROVEMENT from 76.13333333333334 to 79.2\n",
      "Epoch 5/300, loss 18.901320338249207\n",
      "> Test Accuracy = 75.26666666666667 [best = 79.2]\n",
      "Epoch 6/300, loss 18.497240841388702\n",
      "--> IMPROVEMENT from 79.2 to 79.53333333333333\n",
      "Epoch 7/300, loss 17.554348170757294\n",
      "--> IMPROVEMENT from 79.53333333333333 to 82.06666666666666\n",
      "Epoch 8/300, loss 16.545835465192795\n",
      "--> IMPROVEMENT from 82.06666666666666 to 83.0\n",
      "Epoch 9/300, loss 15.348822623491287\n",
      "--> IMPROVEMENT from 83.0 to 83.73333333333333\n",
      "Epoch 10/300, loss 14.271816074848175\n",
      "--> IMPROVEMENT from 83.73333333333333 to 84.06666666666666\n",
      "Epoch 11/300, loss 13.446727007627487\n",
      "--> IMPROVEMENT from 84.06666666666666 to 84.93333333333334\n",
      "Epoch 12/300, loss 12.852170050144196\n",
      "--> IMPROVEMENT from 84.93333333333334 to 85.8\n",
      "Epoch 13/300, loss 12.293053656816483\n",
      "--> IMPROVEMENT from 85.8 to 86.2\n",
      "Epoch 14/300, loss 11.815901905298233\n",
      "--> IMPROVEMENT from 86.2 to 86.53333333333333\n",
      "Epoch 15/300, loss 11.59213638305664\n",
      "> Test Accuracy = 86.46666666666667 [best = 86.53333333333333]\n",
      "Epoch 16/300, loss 11.557713031768799\n",
      "--> IMPROVEMENT from 86.53333333333333 to 86.66666666666667\n",
      "Epoch 17/300, loss 11.83562183380127\n",
      "> Test Accuracy = 86.13333333333334 [best = 86.66666666666667]\n",
      "Epoch 18/300, loss 11.38770180940628\n",
      "> Test Accuracy = 86.6 [best = 86.66666666666667]\n",
      "Epoch 19/300, loss 11.2604421377182\n",
      "> Test Accuracy = 86.6 [best = 86.66666666666667]\n",
      "Epoch 20/300, loss 11.194639384746552\n",
      "> Test Accuracy = 86.66666666666667 [best = 86.66666666666667]\n",
      "Epoch 21/300, loss 11.14624759554863\n",
      "> Test Accuracy = 86.26666666666667 [best = 86.66666666666667]\n",
      "Epoch 22/300, loss 10.987912684679031\n",
      "> Test Accuracy = 86.26666666666667 [best = 86.66666666666667]\n",
      "Epoch 23/300, loss 10.848406493663788\n",
      "> Test Accuracy = 86.66666666666667 [best = 86.66666666666667]\n",
      "Epoch 24/300, loss 10.716065630316734\n",
      "--> IMPROVEMENT from 86.66666666666667 to 87.06666666666666\n",
      "Epoch 25/300, loss 10.806586980819702\n",
      "> Test Accuracy = 86.66666666666667 [best = 87.06666666666666]\n",
      "Epoch 26/300, loss 10.740902483463287\n",
      "> Test Accuracy = 86.6 [best = 87.06666666666666]\n",
      "Epoch 27/300, loss 10.635026440024376\n",
      "--> IMPROVEMENT from 87.06666666666666 to 87.4\n",
      "Epoch 28/300, loss 10.519952267408371\n",
      "--> IMPROVEMENT from 87.4 to 87.46666666666667\n",
      "Epoch 29/300, loss 10.443438023328781\n",
      "> Test Accuracy = 87.33333333333333 [best = 87.46666666666667]\n",
      "Epoch 30/300, loss 10.46755675971508\n",
      "> Test Accuracy = 86.53333333333333 [best = 87.46666666666667]\n",
      "Epoch 31/300, loss 10.432177439332008\n",
      "> Test Accuracy = 86.93333333333334 [best = 87.46666666666667]\n",
      "Epoch 32/300, loss 10.285163179039955\n",
      "> Test Accuracy = 87.46666666666667 [best = 87.46666666666667]\n",
      "Epoch 33/300, loss 10.139458537101746\n",
      "--> IMPROVEMENT from 87.46666666666667 to 87.66666666666667\n",
      "Epoch 34/300, loss 10.164131462574005\n",
      "--> IMPROVEMENT from 87.66666666666667 to 87.8\n",
      "Epoch 35/300, loss 10.045562744140625\n",
      "> Test Accuracy = 87.73333333333333 [best = 87.8]\n",
      "Epoch 36/300, loss 10.18805681169033\n",
      "> Test Accuracy = 87.53333333333333 [best = 87.8]\n",
      "Epoch 37/300, loss 10.03842018544674\n",
      "--> IMPROVEMENT from 87.8 to 87.93333333333334\n",
      "Epoch 38/300, loss 10.059176996350288\n",
      "> Test Accuracy = 87.73333333333333 [best = 87.93333333333334]\n",
      "Epoch 39/300, loss 10.297358050942421\n",
      "> Test Accuracy = 87.06666666666666 [best = 87.93333333333334]\n",
      "Epoch 40/300, loss 10.441890299320221\n",
      "> Test Accuracy = 86.73333333333333 [best = 87.93333333333334]\n",
      "Epoch 41/300, loss 10.556738257408142\n",
      "> Test Accuracy = 86.8 [best = 87.93333333333334]\n",
      "Epoch 42/300, loss 10.015626221895218\n",
      "> Test Accuracy = 87.66666666666667 [best = 87.93333333333334]\n",
      "Epoch 43/300, loss 9.852539896965027\n",
      "> Test Accuracy = 87.73333333333333 [best = 87.93333333333334]\n",
      "Epoch 44/300, loss 9.855956837534904\n",
      "> Test Accuracy = 87.66666666666667 [best = 87.93333333333334]\n",
      "Epoch 45/300, loss 9.879666894674301\n",
      "> Test Accuracy = 87.6 [best = 87.93333333333334]\n",
      "Epoch 46/300, loss 9.867673948407173\n",
      "> Test Accuracy = 87.4 [best = 87.93333333333334]\n",
      "Epoch 47/300, loss 9.925553500652313\n",
      "> Test Accuracy = 87.33333333333333 [best = 87.93333333333334]\n",
      "Epoch 48/300, loss 10.067549854516983\n",
      "> Test Accuracy = 86.86666666666666 [best = 87.93333333333334]\n",
      "Epoch 49/300, loss 9.95290493965149\n",
      "> Test Accuracy = 87.26666666666667 [best = 87.93333333333334]\n",
      "Epoch 50/300, loss 9.793528720736504\n",
      "> Test Accuracy = 87.46666666666667 [best = 87.93333333333334]\n",
      "Epoch 51/300, loss 9.854461163282394\n",
      "> Test Accuracy = 87.13333333333334 [best = 87.93333333333334]\n",
      "Epoch 52/300, loss 9.772973775863647\n",
      "> Test Accuracy = 87.4 [best = 87.93333333333334]\n",
      "Epoch 53/300, loss 9.765737235546112\n",
      "> Test Accuracy = 87.8 [best = 87.93333333333334]\n",
      "Epoch 54/300, loss 9.742221638560295\n",
      "--> IMPROVEMENT from 87.93333333333334 to 88.06666666666666\n",
      "Epoch 55/300, loss 9.721875920891762\n",
      "--> IMPROVEMENT from 88.06666666666666 to 88.33333333333333\n",
      "Epoch 56/300, loss 9.683769091963768\n",
      "> Test Accuracy = 88.2 [best = 88.33333333333333]\n",
      "Epoch 57/300, loss 9.682992845773697\n",
      "--> IMPROVEMENT from 88.33333333333333 to 88.4\n",
      "Epoch 58/300, loss 9.53829699754715\n",
      "--> IMPROVEMENT from 88.4 to 88.66666666666667\n",
      "Epoch 59/300, loss 9.512879148125648\n",
      "> Test Accuracy = 88.33333333333333 [best = 88.66666666666667]\n",
      "Epoch 60/300, loss 9.489636793732643\n",
      "> Test Accuracy = 88.46666666666667 [best = 88.66666666666667]\n",
      "Epoch 61/300, loss 9.823880523443222\n",
      "> Test Accuracy = 88.53333333333333 [best = 88.66666666666667]\n",
      "Epoch 62/300, loss 9.980519950389862\n",
      "--> IMPROVEMENT from 88.66666666666667 to 89.0\n",
      "Epoch 63/300, loss 9.658919647336006\n",
      "> Test Accuracy = 88.66666666666667 [best = 89.0]\n",
      "Epoch 64/300, loss 9.702142924070358\n",
      "> Test Accuracy = 88.2 [best = 89.0]\n",
      "Epoch 65/300, loss 9.718058466911316\n",
      "> Test Accuracy = 88.6 [best = 89.0]\n",
      "Epoch 66/300, loss 9.62803065776825\n",
      "> Test Accuracy = 88.93333333333334 [best = 89.0]\n",
      "Epoch 67/300, loss 9.846212193369865\n",
      "--> IMPROVEMENT from 89.0 to 89.06666666666666\n",
      "Epoch 68/300, loss 9.917062789201736\n",
      "> Test Accuracy = 88.86666666666666 [best = 89.06666666666666]\n",
      "Epoch 69/300, loss 10.089050769805908\n",
      "--> IMPROVEMENT from 89.06666666666666 to 89.26666666666667\n",
      "Epoch 70/300, loss 10.201416343450546\n",
      "> Test Accuracy = 88.4 [best = 89.26666666666667]\n",
      "Epoch 71/300, loss 10.612604781985283\n",
      "> Test Accuracy = 87.13333333333334 [best = 89.26666666666667]\n",
      "Epoch 72/300, loss 10.313296094536781\n",
      "> Test Accuracy = 86.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 73/300, loss 10.097932890057564\n",
      "> Test Accuracy = 86.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 74/300, loss 9.9715086966753\n",
      "> Test Accuracy = 85.8 [best = 89.26666666666667]\n",
      "Epoch 75/300, loss 9.9895039498806\n",
      "> Test Accuracy = 85.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 76/300, loss 9.853627622127533\n",
      "> Test Accuracy = 86.4 [best = 89.26666666666667]\n",
      "Epoch 77/300, loss 9.622068881988525\n",
      "> Test Accuracy = 87.13333333333334 [best = 89.26666666666667]\n",
      "Epoch 78/300, loss 9.45312936604023\n",
      "> Test Accuracy = 87.13333333333334 [best = 89.26666666666667]\n",
      "Epoch 79/300, loss 9.475175261497498\n",
      "> Test Accuracy = 87.06666666666666 [best = 89.26666666666667]\n",
      "Epoch 80/300, loss 9.48631577193737\n",
      "> Test Accuracy = 86.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 81/300, loss 9.524976879358292\n",
      "> Test Accuracy = 87.0 [best = 89.26666666666667]\n",
      "Epoch 82/300, loss 9.54676154255867\n",
      "> Test Accuracy = 87.0 [best = 89.26666666666667]\n",
      "Epoch 83/300, loss 9.484702363610268\n",
      "> Test Accuracy = 87.06666666666666 [best = 89.26666666666667]\n",
      "Epoch 84/300, loss 9.455113306641579\n",
      "> Test Accuracy = 87.33333333333333 [best = 89.26666666666667]\n",
      "Epoch 85/300, loss 9.406814455986023\n",
      "> Test Accuracy = 87.2 [best = 89.26666666666667]\n",
      "Epoch 86/300, loss 9.486429199576378\n",
      "> Test Accuracy = 87.06666666666666 [best = 89.26666666666667]\n",
      "Epoch 87/300, loss 9.46452483534813\n",
      "> Test Accuracy = 86.8 [best = 89.26666666666667]\n",
      "Epoch 88/300, loss 9.475238367915154\n",
      "> Test Accuracy = 87.13333333333334 [best = 89.26666666666667]\n",
      "Epoch 89/300, loss 9.37479880452156\n",
      "> Test Accuracy = 87.13333333333334 [best = 89.26666666666667]\n",
      "Epoch 90/300, loss 9.4506556391716\n",
      "> Test Accuracy = 87.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 91/300, loss 9.461804628372192\n",
      "> Test Accuracy = 86.6 [best = 89.26666666666667]\n",
      "Epoch 92/300, loss 9.518389865756035\n",
      "> Test Accuracy = 86.73333333333333 [best = 89.26666666666667]\n",
      "Epoch 93/300, loss 9.458274573087692\n",
      "> Test Accuracy = 87.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 94/300, loss 9.37039102613926\n",
      "> Test Accuracy = 87.2 [best = 89.26666666666667]\n",
      "Epoch 95/300, loss 9.350400730967522\n",
      "> Test Accuracy = 87.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 96/300, loss 9.30134791135788\n",
      "> Test Accuracy = 87.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 97/300, loss 9.25710515677929\n",
      "> Test Accuracy = 87.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 98/300, loss 9.2799481600523\n",
      "> Test Accuracy = 87.8 [best = 89.26666666666667]\n",
      "Epoch 99/300, loss 9.227438509464264\n",
      "> Test Accuracy = 87.8 [best = 89.26666666666667]\n",
      "Epoch 100/300, loss 9.278941810131073\n",
      "> Test Accuracy = 87.86666666666666 [best = 89.26666666666667]\n",
      "Epoch 101/300, loss 9.198350459337234\n",
      "> Test Accuracy = 87.86666666666666 [best = 89.26666666666667]\n",
      "Epoch 102/300, loss 9.218191295862198\n",
      "> Test Accuracy = 87.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 103/300, loss 9.221843212842941\n",
      "> Test Accuracy = 87.73333333333333 [best = 89.26666666666667]\n",
      "Epoch 104/300, loss 9.186793774366379\n",
      "> Test Accuracy = 87.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 105/300, loss 9.218909099698067\n",
      "> Test Accuracy = 87.8 [best = 89.26666666666667]\n",
      "Epoch 106/300, loss 9.187455952167511\n",
      "> Test Accuracy = 87.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 107/300, loss 9.213838398456573\n",
      "> Test Accuracy = 87.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 108/300, loss 9.11084258556366\n",
      "> Test Accuracy = 87.73333333333333 [best = 89.26666666666667]\n",
      "Epoch 109/300, loss 9.149464964866638\n",
      "> Test Accuracy = 88.06666666666666 [best = 89.26666666666667]\n",
      "Epoch 110/300, loss 9.192321464419365\n",
      "> Test Accuracy = 87.86666666666666 [best = 89.26666666666667]\n",
      "Epoch 111/300, loss 9.249429166316986\n",
      "> Test Accuracy = 87.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 112/300, loss 9.209968939423561\n",
      "> Test Accuracy = 87.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 113/300, loss 9.137907058000565\n",
      "> Test Accuracy = 87.8 [best = 89.26666666666667]\n",
      "Epoch 114/300, loss 9.09874102473259\n",
      "> Test Accuracy = 87.8 [best = 89.26666666666667]\n",
      "Epoch 115/300, loss 9.078881859779358\n",
      "> Test Accuracy = 87.8 [best = 89.26666666666667]\n",
      "Epoch 116/300, loss 9.150721594691277\n",
      "> Test Accuracy = 87.6 [best = 89.26666666666667]\n",
      "Epoch 117/300, loss 9.225089028477669\n",
      "> Test Accuracy = 87.4 [best = 89.26666666666667]\n",
      "Epoch 118/300, loss 9.122633770108223\n",
      "> Test Accuracy = 87.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 119/300, loss 9.03773245215416\n",
      "> Test Accuracy = 88.0 [best = 89.26666666666667]\n",
      "Epoch 120/300, loss 9.064717814326286\n",
      "> Test Accuracy = 87.86666666666666 [best = 89.26666666666667]\n",
      "Epoch 121/300, loss 9.082210510969162\n",
      "> Test Accuracy = 87.86666666666666 [best = 89.26666666666667]\n",
      "Epoch 122/300, loss 9.07819564640522\n",
      "> Test Accuracy = 87.86666666666666 [best = 89.26666666666667]\n",
      "Epoch 123/300, loss 9.075334250926971\n",
      "> Test Accuracy = 87.73333333333333 [best = 89.26666666666667]\n",
      "Epoch 124/300, loss 8.975639134645462\n",
      "> Test Accuracy = 87.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 125/300, loss 8.956354483962059\n",
      "> Test Accuracy = 87.86666666666666 [best = 89.26666666666667]\n",
      "Epoch 126/300, loss 8.95305122435093\n",
      "> Test Accuracy = 88.06666666666666 [best = 89.26666666666667]\n",
      "Epoch 127/300, loss 8.994307711720467\n",
      "> Test Accuracy = 87.86666666666666 [best = 89.26666666666667]\n",
      "Epoch 128/300, loss 9.029713898897171\n",
      "> Test Accuracy = 87.73333333333333 [best = 89.26666666666667]\n",
      "Epoch 129/300, loss 8.990763261914253\n",
      "> Test Accuracy = 87.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 130/300, loss 9.023812353610992\n",
      "> Test Accuracy = 87.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 131/300, loss 9.021421313285828\n",
      "> Test Accuracy = 87.73333333333333 [best = 89.26666666666667]\n",
      "Epoch 132/300, loss 9.097286522388458\n",
      "> Test Accuracy = 87.6 [best = 89.26666666666667]\n",
      "Epoch 133/300, loss 9.156878635287285\n",
      "> Test Accuracy = 87.2 [best = 89.26666666666667]\n",
      "Epoch 134/300, loss 9.199485152959824\n",
      "> Test Accuracy = 87.33333333333333 [best = 89.26666666666667]\n",
      "Epoch 135/300, loss 9.138383716344833\n",
      "> Test Accuracy = 87.6 [best = 89.26666666666667]\n",
      "Epoch 136/300, loss 9.054094031453133\n",
      "> Test Accuracy = 87.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 137/300, loss 8.966828510165215\n",
      "> Test Accuracy = 87.8 [best = 89.26666666666667]\n",
      "Epoch 138/300, loss 8.997586354613304\n",
      "> Test Accuracy = 87.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 139/300, loss 9.014144256711006\n",
      "> Test Accuracy = 87.6 [best = 89.26666666666667]\n",
      "Epoch 140/300, loss 8.94739754498005\n",
      "> Test Accuracy = 87.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 141/300, loss 9.053590402007103\n",
      "> Test Accuracy = 87.6 [best = 89.26666666666667]\n",
      "Epoch 142/300, loss 9.06194843351841\n",
      "> Test Accuracy = 87.73333333333333 [best = 89.26666666666667]\n",
      "Epoch 143/300, loss 9.176771804690361\n",
      "> Test Accuracy = 87.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 144/300, loss 8.998123735189438\n",
      "> Test Accuracy = 87.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 145/300, loss 8.918058171868324\n",
      "> Test Accuracy = 88.13333333333334 [best = 89.26666666666667]\n",
      "Epoch 146/300, loss 8.78687071800232\n",
      "> Test Accuracy = 88.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 147/300, loss 8.755832999944687\n",
      "> Test Accuracy = 88.2 [best = 89.26666666666667]\n",
      "Epoch 148/300, loss 8.76458689570427\n",
      "> Test Accuracy = 88.13333333333334 [best = 89.26666666666667]\n",
      "Epoch 149/300, loss 8.789994418621063\n",
      "> Test Accuracy = 88.06666666666666 [best = 89.26666666666667]\n",
      "Epoch 150/300, loss 8.815219983458519\n",
      "> Test Accuracy = 88.2 [best = 89.26666666666667]\n",
      "Epoch 151/300, loss 8.82493893802166\n",
      "> Test Accuracy = 88.06666666666666 [best = 89.26666666666667]\n",
      "Epoch 152/300, loss 8.876345738768578\n",
      "> Test Accuracy = 88.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 153/300, loss 8.82708452641964\n",
      "> Test Accuracy = 88.13333333333334 [best = 89.26666666666667]\n",
      "Epoch 154/300, loss 8.775459676980972\n",
      "> Test Accuracy = 88.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 155/300, loss 8.763482093811035\n",
      "> Test Accuracy = 88.06666666666666 [best = 89.26666666666667]\n",
      "Epoch 156/300, loss 8.805637910962105\n",
      "> Test Accuracy = 88.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 157/300, loss 8.801822394132614\n",
      "> Test Accuracy = 88.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 158/300, loss 8.789964407682419\n",
      "> Test Accuracy = 88.2 [best = 89.26666666666667]\n",
      "Epoch 159/300, loss 8.728348806500435\n",
      "> Test Accuracy = 88.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 160/300, loss 8.721117988228798\n",
      "> Test Accuracy = 88.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 161/300, loss 8.703793704509735\n",
      "> Test Accuracy = 88.33333333333333 [best = 89.26666666666667]\n",
      "Epoch 162/300, loss 8.804969727993011\n",
      "> Test Accuracy = 88.13333333333334 [best = 89.26666666666667]\n",
      "Epoch 163/300, loss 8.864591166377068\n",
      "> Test Accuracy = 88.06666666666666 [best = 89.26666666666667]\n",
      "Epoch 164/300, loss 8.929874137043953\n",
      "> Test Accuracy = 88.13333333333334 [best = 89.26666666666667]\n",
      "Epoch 165/300, loss 8.957993403077126\n",
      "> Test Accuracy = 87.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 166/300, loss 8.973672777414322\n",
      "> Test Accuracy = 88.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 167/300, loss 9.023823827505112\n",
      "> Test Accuracy = 88.2 [best = 89.26666666666667]\n",
      "Epoch 168/300, loss 8.893162444233894\n",
      "> Test Accuracy = 87.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 169/300, loss 9.007058382034302\n",
      "> Test Accuracy = 89.0 [best = 89.26666666666667]\n",
      "Epoch 170/300, loss 8.760593682527542\n",
      "> Test Accuracy = 88.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 171/300, loss 8.798048093914986\n",
      "> Test Accuracy = 87.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 172/300, loss 8.841866999864578\n",
      "> Test Accuracy = 87.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 173/300, loss 8.902930840849876\n",
      "> Test Accuracy = 88.0 [best = 89.26666666666667]\n",
      "Epoch 174/300, loss 8.923098370432854\n",
      "> Test Accuracy = 88.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 175/300, loss 9.088275507092476\n",
      "> Test Accuracy = 87.6 [best = 89.26666666666667]\n",
      "Epoch 176/300, loss 9.1060229241848\n",
      "> Test Accuracy = 88.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 177/300, loss 8.713889211416245\n",
      "> Test Accuracy = 88.4 [best = 89.26666666666667]\n",
      "Epoch 178/300, loss 8.547923564910889\n",
      "> Test Accuracy = 88.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 179/300, loss 8.571514546871185\n",
      "> Test Accuracy = 88.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 180/300, loss 8.643620625138283\n",
      "> Test Accuracy = 88.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 181/300, loss 8.612582057714462\n",
      "> Test Accuracy = 88.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 182/300, loss 8.564732700586319\n",
      "> Test Accuracy = 88.4 [best = 89.26666666666667]\n",
      "Epoch 183/300, loss 8.616992697119713\n",
      "> Test Accuracy = 88.0 [best = 89.26666666666667]\n",
      "Epoch 184/300, loss 8.612013801932335\n",
      "> Test Accuracy = 88.6 [best = 89.26666666666667]\n",
      "Epoch 185/300, loss 8.687827199697495\n",
      "> Test Accuracy = 88.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 186/300, loss 8.654309317469597\n",
      "> Test Accuracy = 88.0 [best = 89.26666666666667]\n",
      "Epoch 187/300, loss 9.002197504043579\n",
      "> Test Accuracy = 88.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 188/300, loss 8.670671254396439\n",
      "> Test Accuracy = 88.4 [best = 89.26666666666667]\n",
      "Epoch 189/300, loss 8.586027666926384\n",
      "> Test Accuracy = 88.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 190/300, loss 8.604684486985207\n",
      "> Test Accuracy = 88.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 191/300, loss 8.540108010172844\n",
      "> Test Accuracy = 88.6 [best = 89.26666666666667]\n",
      "Epoch 192/300, loss 8.57755422592163\n",
      "> Test Accuracy = 88.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 193/300, loss 8.600992143154144\n",
      "> Test Accuracy = 88.6 [best = 89.26666666666667]\n",
      "Epoch 194/300, loss 8.57934294641018\n",
      "> Test Accuracy = 88.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 195/300, loss 8.594444885849953\n",
      "> Test Accuracy = 88.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 196/300, loss 8.650506660342216\n",
      "> Test Accuracy = 88.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 197/300, loss 8.559807226061821\n",
      "> Test Accuracy = 88.2 [best = 89.26666666666667]\n",
      "Epoch 198/300, loss 8.557752653956413\n",
      "> Test Accuracy = 88.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 199/300, loss 8.588548600673676\n",
      "> Test Accuracy = 88.4 [best = 89.26666666666667]\n",
      "Epoch 200/300, loss 8.680616915225983\n",
      "> Test Accuracy = 88.4 [best = 89.26666666666667]\n",
      "Epoch 201/300, loss 8.60081434249878\n",
      "> Test Accuracy = 88.33333333333333 [best = 89.26666666666667]\n",
      "Epoch 202/300, loss 8.623308792710304\n",
      "> Test Accuracy = 88.4 [best = 89.26666666666667]\n",
      "Epoch 203/300, loss 8.651943355798721\n",
      "> Test Accuracy = 88.33333333333333 [best = 89.26666666666667]\n",
      "Epoch 204/300, loss 8.642479568719864\n",
      "> Test Accuracy = 88.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 205/300, loss 8.593996956944466\n",
      "> Test Accuracy = 88.4 [best = 89.26666666666667]\n",
      "Epoch 206/300, loss 8.638930901885033\n",
      "> Test Accuracy = 88.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 207/300, loss 8.596607849001884\n",
      "> Test Accuracy = 88.33333333333333 [best = 89.26666666666667]\n",
      "Epoch 208/300, loss 8.610874995589256\n",
      "> Test Accuracy = 88.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 209/300, loss 8.628026157617569\n",
      "> Test Accuracy = 88.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 210/300, loss 8.550830438733101\n",
      "> Test Accuracy = 88.73333333333333 [best = 89.26666666666667]\n",
      "Epoch 211/300, loss 8.510119676589966\n",
      "> Test Accuracy = 88.4 [best = 89.26666666666667]\n",
      "Epoch 212/300, loss 8.616655513644218\n",
      "> Test Accuracy = 88.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 213/300, loss 8.532082542777061\n",
      "> Test Accuracy = 88.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 214/300, loss 8.506761237978935\n",
      "> Test Accuracy = 88.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 215/300, loss 8.465405106544495\n",
      "> Test Accuracy = 88.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 216/300, loss 8.507826641201973\n",
      "> Test Accuracy = 88.73333333333333 [best = 89.26666666666667]\n",
      "Epoch 217/300, loss 8.465908154845238\n",
      "> Test Accuracy = 88.8 [best = 89.26666666666667]\n",
      "Epoch 218/300, loss 8.497054263949394\n",
      "> Test Accuracy = 88.73333333333333 [best = 89.26666666666667]\n",
      "Epoch 219/300, loss 8.598652571439743\n",
      "> Test Accuracy = 88.06666666666666 [best = 89.26666666666667]\n",
      "Epoch 220/300, loss 8.515585780143738\n",
      "> Test Accuracy = 88.6 [best = 89.26666666666667]\n",
      "Epoch 221/300, loss 8.492319077253342\n",
      "> Test Accuracy = 88.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 222/300, loss 8.457077413797379\n",
      "> Test Accuracy = 88.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 223/300, loss 8.435438007116318\n",
      "> Test Accuracy = 88.46666666666667 [best = 89.26666666666667]\n",
      "Epoch 224/300, loss 8.460883095860481\n",
      "> Test Accuracy = 88.53333333333333 [best = 89.26666666666667]\n",
      "Epoch 225/300, loss 8.43844823539257\n",
      "> Test Accuracy = 88.73333333333333 [best = 89.26666666666667]\n",
      "Epoch 226/300, loss 8.397888869047165\n",
      "> Test Accuracy = 88.86666666666666 [best = 89.26666666666667]\n",
      "Epoch 227/300, loss 8.365188777446747\n",
      "> Test Accuracy = 88.73333333333333 [best = 89.26666666666667]\n",
      "Epoch 228/300, loss 8.356627270579338\n",
      "> Test Accuracy = 88.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 229/300, loss 8.364191204309464\n",
      "> Test Accuracy = 88.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 230/300, loss 8.385826751589775\n",
      "> Test Accuracy = 88.66666666666667 [best = 89.26666666666667]\n",
      "Epoch 231/300, loss 8.335607901215553\n",
      "> Test Accuracy = 88.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 232/300, loss 8.368518397212029\n",
      "> Test Accuracy = 89.06666666666666 [best = 89.26666666666667]\n",
      "Epoch 233/300, loss 8.36370262503624\n",
      "> Test Accuracy = 88.86666666666666 [best = 89.26666666666667]\n",
      "Epoch 234/300, loss 8.363861411809921\n",
      "> Test Accuracy = 88.86666666666666 [best = 89.26666666666667]\n",
      "Epoch 235/300, loss 8.420406937599182\n",
      "> Test Accuracy = 89.0 [best = 89.26666666666667]\n",
      "Epoch 236/300, loss 8.35030221939087\n",
      "> Test Accuracy = 89.26666666666667 [best = 89.26666666666667]\n",
      "Epoch 237/300, loss 8.352202609181404\n",
      "> Test Accuracy = 88.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 238/300, loss 8.358797416090965\n",
      "> Test Accuracy = 89.06666666666666 [best = 89.26666666666667]\n",
      "Epoch 239/300, loss 8.300625815987587\n",
      "> Test Accuracy = 88.86666666666666 [best = 89.26666666666667]\n",
      "Epoch 240/300, loss 8.355860084295273\n",
      "> Test Accuracy = 88.93333333333334 [best = 89.26666666666667]\n",
      "Epoch 241/300, loss 8.434106856584549\n",
      "> Test Accuracy = 88.8 [best = 89.26666666666667]\n",
      "Epoch 242/300, loss 8.531697377562523\n",
      "> Test Accuracy = 88.8 [best = 89.26666666666667]\n",
      "Epoch 243/300, loss 8.354139223694801\n",
      "> Test Accuracy = 89.0 [best = 89.26666666666667]\n",
      "Epoch 244/300, loss 8.509781584143639\n",
      "--> IMPROVEMENT from 89.26666666666667 to 89.4\n",
      "Epoch 245/300, loss 8.55712103843689\n",
      "> Test Accuracy = 88.93333333333334 [best = 89.4]\n",
      "Epoch 246/300, loss 8.540195941925049\n",
      "> Test Accuracy = 88.53333333333333 [best = 89.4]\n",
      "Epoch 247/300, loss 8.45570920407772\n",
      "> Test Accuracy = 87.86666666666666 [best = 89.4]\n",
      "Epoch 248/300, loss 8.684650644659996\n",
      "> Test Accuracy = 86.73333333333333 [best = 89.4]\n",
      "Epoch 249/300, loss 8.562367871403694\n",
      "> Test Accuracy = 87.93333333333334 [best = 89.4]\n",
      "Epoch 250/300, loss 8.481638848781586\n",
      "> Test Accuracy = 88.8 [best = 89.4]\n",
      "Epoch 251/300, loss 8.412877932190895\n",
      "> Test Accuracy = 89.0 [best = 89.4]\n",
      "Epoch 252/300, loss 8.412466496229172\n",
      "> Test Accuracy = 88.86666666666666 [best = 89.4]\n",
      "Epoch 253/300, loss 8.40668135881424\n",
      "> Test Accuracy = 89.06666666666666 [best = 89.4]\n",
      "Epoch 254/300, loss 8.387675866484642\n",
      "> Test Accuracy = 88.53333333333333 [best = 89.4]\n",
      "Epoch 255/300, loss 8.391994714736938\n",
      "> Test Accuracy = 89.26666666666667 [best = 89.4]\n",
      "Epoch 256/300, loss 8.274288073182106\n",
      "> Test Accuracy = 89.2 [best = 89.4]\n",
      "Epoch 257/300, loss 8.254220679402351\n",
      "> Test Accuracy = 88.93333333333334 [best = 89.4]\n",
      "Epoch 258/300, loss 8.319316521286964\n",
      "> Test Accuracy = 89.06666666666666 [best = 89.4]\n",
      "Epoch 259/300, loss 8.297502890229225\n",
      "> Test Accuracy = 89.2 [best = 89.4]\n",
      "Epoch 260/300, loss 8.278610587120056\n",
      "> Test Accuracy = 89.13333333333334 [best = 89.4]\n",
      "Epoch 261/300, loss 8.26776371896267\n",
      "> Test Accuracy = 89.0 [best = 89.4]\n",
      "Epoch 262/300, loss 8.342382475733757\n",
      "> Test Accuracy = 89.0 [best = 89.4]\n",
      "Epoch 263/300, loss 8.24284303188324\n",
      "> Test Accuracy = 88.86666666666666 [best = 89.4]\n",
      "Epoch 264/300, loss 8.312083706259727\n",
      "> Test Accuracy = 88.73333333333333 [best = 89.4]\n",
      "Epoch 265/300, loss 8.307829096913338\n",
      "> Test Accuracy = 88.66666666666667 [best = 89.4]\n",
      "Epoch 266/300, loss 8.324555933475494\n",
      "> Test Accuracy = 89.26666666666667 [best = 89.4]\n",
      "Epoch 267/300, loss 8.29192627966404\n",
      "> Test Accuracy = 89.0 [best = 89.4]\n",
      "Epoch 268/300, loss 8.288820207118988\n",
      "> Test Accuracy = 89.13333333333334 [best = 89.4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# print(f'\\n{len(batch_in_pd0) = }\\n{len(batch_in_pd1) = }\\n{batch_target.shape = }')\u001b[39;00m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 31\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_in_pd0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_in_pd1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(result, batch_target)\n\u001b[1;32m     33\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 40\u001b[0m, in \u001b[0;36mPerslay_KTH.forward\u001b[0;34m(self, batch_pers_0, batch_pers_1)\u001b[0m\n\u001b[1;32m     37\u001b[0m p0, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(p0, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# same for p1\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m p1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m# not always with same len of p0\u001b[39;00m\n\u001b[1;32m     42\u001b[0m p1_shape \u001b[38;5;241m=\u001b[39m p1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# number of elemnts in H1 persistence\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p1_shape\u001b[38;5;241m<\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "model = Perslay_KTH(hidden_size = 25)\n",
    "\n",
    "# Learning rate and loss function.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# scheduler = StepLR(optimizer, step_size=70, gamma=0.7)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 300\n",
    "recorded_loss = torch.zeros(epochs)\n",
    "\n",
    "best_acc = 0.0 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    # for batch in tqdm(train_batched_data):\n",
    "    for batch in train_batched_data:\n",
    "\n",
    "        batch_in_pd0 = [sample['persist_0'] for sample in batch] # get tersor of persistence\n",
    "\n",
    "        batch_in_pd1 = [sample['persist_1'] for sample in batch] # get tersor of persistence\n",
    "\n",
    "        batch_target = torch.tensor([sample['id_class'] for sample in batch]) # get target labels\n",
    "        # print(f'\\n{len(batch_in_pd0) = }\\n{len(batch_in_pd1) = }\\n{batch_target.shape = }')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        result = model(batch_in_pd0, batch_in_pd1)\n",
    "        loss = loss_function(result, batch_target)\n",
    "        total_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "    \n",
    "    recorded_loss[epoch] = total_loss\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, loss {total_loss}\")\n",
    "\n",
    "    test_acc = test_model(model, test_batched_data, if_plot=False)\n",
    "    if best_acc < test_acc:\n",
    "        if test_acc > 70.0:\n",
    "            torch.save(model.state_dict(), f'./Perslay_models/try_acc_{test_acc}.pth')\n",
    "        print(f'--> IMPROVEMENT from {best_acc} to {test_acc}')\n",
    "        best_acc = test_acc\n",
    "    else:\n",
    "        print(f'> Test Accuracy = {test_acc} [best = {best_acc}]')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recorded_loss)\n",
    "#ax.set_ylim([0, 1])\n",
    "plt.show()\n",
    "print(f\"Final loss is {recorded_loss[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './Perslay_models/try0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a model named 'model' and a test dataloader named 'test_loader'\n",
    "test_model(model, train_batched_data, if_plot=True)\n",
    "test_model(model, test_batched_data, if_plot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Learning rate and loss function.\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# ema_model = torch.optim.swa_utils.AveragedModel(model, \\\n",
    "#             multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.))\n",
    "# scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 3\n",
    "recorded_loss = torch.zeros(epochs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recorded_loss)\n",
    "#ax.set_ylim([0, 1])\n",
    "plt.show()\n",
    "print(f\"Final loss is {recorded_loss[-1]}\")\n",
    "\n",
    "# Assuming you have a model named 'model' and a test dataloader named 'test_loader'\n",
    "pred_train, true_train = test_model(model, pds_train)\n",
    "pred_test, true_test = test_model(model, pds_test)\n",
    "\n",
    "cm = confusion_matrix(true_train, pred_train)\n",
    "classes = [1,2,3,4,5]\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('TRAIN Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "plt.cla()\n",
    "cm = confusion_matrix(true_test, pred_test)\n",
    "classes = [1,2,3,4,5]\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('TEST Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advtopo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
