{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import functools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import gudhi as gd\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64) \n",
    "import torch.nn as nn\n",
    "\n",
    "def rand(shape, low, high):\n",
    "    \"\"\"Tensor of random numbers, uniformly distributed on [low, high].\"\"\"\n",
    "    return torch.rand(shape) * (high - low) + low\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Return the number of trainable parameters of a model (the total number of scalars).\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I WANT TO COMPARE CLOUDS WITH 1K POINTS WITH THE SAME STARTING POINT\n",
    "def generate_orbits(n_points_per_orbit = 1000, params = [2.5, 3.5, 4.0, 4.1, 4.3], same_init_point = True):\n",
    "    # create point clouds \n",
    "    ORBITS = np.zeros([len(params), n_points_per_orbit, 2])\n",
    "    xcur_0, ycur_0 = np.random.rand(), np.random.rand() # not necesary to save the first one\n",
    "    for id_pc, param in enumerate(params): # id_point_cloud\n",
    "        if same_init_point:\n",
    "            xcur, ycur = xcur_0, ycur_0 # not necesary to save the first one\n",
    "        else:\n",
    "            xcur, ycur =np.random.rand(), np.random.rand()\n",
    "        for id_pt in range(n_points_per_orbit): # id_point\n",
    "            xcur = (xcur + param * ycur * (1. - ycur)) % 1\n",
    "            ycur = (ycur + param * xcur * (1. - xcur)) % 1\n",
    "            ORBITS[id_pc, id_pt, :] = [xcur, ycur]\n",
    "    return ORBITS\n",
    "\n",
    "# function from [len(params), n_points, 2] to alpha-complex and persistence diagram\n",
    "# to create PDs we need to: points -> skeleton(ac) -> simplex(st) -> persistence(pers)\n",
    "# for each element of the dataset we'll have len(params) PDs to be compared\n",
    "def extract_PD(cloud, id_class):\n",
    "    \"\"\"extract a dict \n",
    "\n",
    "    Args:\n",
    "        cloud (_type_): array [1000,2] composing th ewhole point cloud\n",
    "        id_class (_type_): index about the class of membership\n",
    "\n",
    "    Returns:\n",
    "        dict: with keys ['persist_0','persist_1','id_class']\n",
    "    \"\"\"\n",
    "    # for every point cloud we create a dictionary storing the label and its persistence\n",
    "    # usage of dictionary to store each other possible data linked to the point clous\n",
    "    ac = gd.AlphaComplex(points=cloud)\n",
    "    st = ac.create_simplex_tree()\n",
    "    pers = st.persistence()\n",
    "    #? EXTENDED PERSISTENCE?\n",
    "    #! TRANSPOSE TO HAVE THEN [BATCH SIZE, 2, NUM POINTS]\n",
    "    pers_0 = np.array(st.persistence_intervals_in_dimension(0)).transpose()\n",
    "    pers_1 = np.array(st.persistence_intervals_in_dimension(1)).transpose()\n",
    "    pers_dict = {\n",
    "        'cloud': cloud, #* UNCOMMENT THE LINE IF YOU WANT TO VISUALIZE POINT CLOUDS LATER\n",
    "        # 'skeleton': ac, # no more used\n",
    "        # 'complex': st, # used for bottleneck distance\n",
    "        'persist_0': pers_0[:,:-1], # removing the last barcode, the one with inf\n",
    "        'persist_1': pers_1, # here we should never have inf, since [0,1]^2 is compact/bounded  \n",
    "        # 'persist': pers, # actual PD\n",
    "        'id_class': id_class # label for classification\n",
    "    }\n",
    "    return pers_dict\n",
    "\n",
    "# \n",
    "def gaussian_transformation(pd):\n",
    "    # Applied in the model after preproc size\n",
    "    # I'm embedding the pair [birth, death] in R^15 \n",
    "    embs = np.apply_along_axis(gamma_p, axis=1, arr=pd)\n",
    "    return torch.tensor(embs)\n",
    "\n",
    "def gamma_p(p):\n",
    "    # params of gaussian_transformation\n",
    "    ts = torch.tensor([[0., 0.], \\\n",
    "          [0.25, 0.], [0.25, 0.25], \\\n",
    "          [0.5, 0.], [0.5, 0.25], [0.5, 0.5], \\\n",
    "          [0.75, 0.], [0.75, 0.25], [0.75, 0.5], [0.75, 0.75], \\\n",
    "          [1., 0.], [1., 0.25], [1., 0.5], [1., 0.75], [1., 1.]])\n",
    "    sigma = 0.2\n",
    "    # single point computaions for gaussian transformation\n",
    "    squared_distances = torch.pow(ts - p, 2).sum(dim=1)\n",
    "    emb = -squared_distances/(2*sigma**2)\n",
    "    emb = torch.exp(emb)\n",
    "    return emb\n",
    "\n",
    "def preproc_prom(tens, prom):\n",
    "    diffs = tens[1] - tens[0]\n",
    "    sorted_diffs = torch.argsort(diffs, descending=True)\n",
    "    sorted_tens = tens[:,sorted_diffs]\n",
    "    return sorted_tens[:,:prom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 4.5783e-01, 2.0961e-01, 4.3937e-02, 2.0116e-02, 1.9305e-03,\n",
       "         8.8383e-04, 4.0465e-04, 3.8833e-05, 7.8115e-07, 3.7267e-06, 1.7062e-06,\n",
       "         1.6374e-07, 3.2937e-09, 1.3888e-11],\n",
       "        [3.7267e-06, 8.8383e-04, 4.0465e-04, 4.3937e-02, 2.0116e-02, 1.9305e-03,\n",
       "         4.5783e-01, 2.0961e-01, 2.0116e-02, 4.0465e-04, 1.0000e+00, 4.5783e-01,\n",
       "         4.3937e-02, 8.8383e-04, 3.7267e-06]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.rand(6, 2)\n",
    "points = torch.tensor([[0.,0.], [1.,0.]])\n",
    "gaussian_transformation(points)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create TRAIN Point Clouds: 100%|██████████| 700/700 [00:37<00:00, 18.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_batched_data) = 28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create TEST Point Clouds: 100%|██████████| 300/300 [00:16<00:00, 17.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_batched_data) = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### FULL DATA GENERATION \n",
    "# (~2 mins) ---------------------------------------\n",
    "\n",
    "# hyper params\n",
    "n_points = 1000\n",
    "params = [2.5, 3.5, 4.0, 4.1, 4.3]\n",
    "same_init_point = True\n",
    "n_seq_per_dataset = [700, 300] # I want [i, len(params), n_points, 2]\n",
    "\n",
    "batch_size = 128\n",
    "extended_pers = False\n",
    "k_pd_preproc = 500\n",
    "\n",
    "# init list fo persistence diagrams\n",
    "pds_train = []\n",
    "pds_test = []\n",
    "\n",
    "# TRAIN \n",
    "for i in tqdm(range(n_seq_per_dataset[0]), desc='Create TRAIN Point Clouds'):\n",
    "    ORBS = generate_orbits(n_points, params, same_init_point) # CREATE THE 5 POINT CLOUDS\n",
    "    for j in range(ORBS.shape[0]):\n",
    "        ij_pers = extract_PD(ORBS[j,:,:], j) # EXTRACT PDs\n",
    "        pds_train.append(ij_pers) # STORE IN THE LIST pds_train\n",
    "\n",
    "train_batched_data = [] # BATCHING DATA FOR THE NN\n",
    "batching = len(pds_train)//batch_size\n",
    "for i in range(batching):\n",
    "    train_batched_data.append(pds_train[i*batch_size:(i+1)*batch_size])\n",
    "# check if we have to add the last batch\n",
    "if batching*batch_size != len(pds_train):\n",
    "    train_batched_data.append(pds_train[batching*batch_size:])\n",
    "print(f'{len(train_batched_data) = }\\n')\n",
    "    \n",
    "\n",
    "# TEST\n",
    "for i in tqdm(range(n_seq_per_dataset[1]), desc='Create TEST Point Clouds'):\n",
    "    ORBS = generate_orbits(n_points, params, same_init_point) # CREATE THE 5 POINT CLOUDS\n",
    "    for j in range(ORBS.shape[0]):\n",
    "        ij_pers = extract_PD(ORBS[j,:,:], j) # EXTRACT PDs\n",
    "        pds_test.append(ij_pers) # STORE IN THE LIST pds_test\n",
    "\n",
    "test_batched_data = [] # BATCHING DATA FOR THE NN\n",
    "batching = len(pds_test)//batch_size\n",
    "for i in range(batching):\n",
    "    test_batched_data.append(pds_test[i*batch_size:(i+1)*batch_size])\n",
    "# check if we have to add the last batch\n",
    "if batching*batch_size != len(pds_test):\n",
    "    test_batched_data.append(pds_test[batching*batch_size:])\n",
    "print(f'{len(test_batched_data) = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersImage_KTH(nn.Module):\n",
    "    def __init__(self, hidden_size:int = 10, alpha_0:bool = True, alpha_1:bool = True, prom:int = 500, top_k:int = 5, using_len_p1:bool = False):\n",
    "        super().__init__()\n",
    "        self.a0 = alpha_0\n",
    "        self.a1 = alpha_1\n",
    "        self.prom = prom\n",
    "        self.top_k = top_k\n",
    "        self.num_classes = 5\n",
    "        self.using_len_p1 = using_len_p1\n",
    "\n",
    "        self.ds_0_a = nn.Linear(15,25)\n",
    "        self.relu_0 = torch.nn.ReLU()\n",
    "        self.ds_0_b = nn.Linear(25,hidden_size)\n",
    "        # self.ds_0_c = DeepSetLayer(10,5)\n",
    "\n",
    "        self.ds_1_a = nn.Linear(15,25)\n",
    "        self.relu_1 = torch.nn.ReLU()\n",
    "        self.ds_1_b = nn.Linear(25,hidden_size)\n",
    "        # self.ds_1_c = DeepSetLayer(10,5)\n",
    "\n",
    "        if using_len_p1:\n",
    "            self.linear_dim_H1 = nn.Linear(1,self.top_k)\n",
    "            self.linear_labels = nn.Linear(self.top_k*(hidden_size*2+1), self.num_classes)\n",
    "        else:\n",
    "            self.linear_labels = nn.Linear(self.top_k*(hidden_size*2), self.num_classes)\n",
    "        \n",
    "        self.name = f'{hidden_size}_{alpha_0}_{alpha_1}_{prom}_{top_k}_{using_len_p1}' # list of params\n",
    "\n",
    "\n",
    "    def forward(self, batch_pers_0, batch_pers_1):\n",
    "        labels = False\n",
    "        for p0, p1 in zip(batch_pers_0, batch_pers_1):# one PD at the time due to different cardinality betweeen different H1 barcodes\n",
    "            # I want to select the self.prom longest barcodes\n",
    "            # print(f'{p0.max(), p0.min() = }')\n",
    "            p0 = torch.tensor(p0)*100 #? rescaling\n",
    "            # print(f'{p0.max(), p0.min() = }')\n",
    "            p0 = preproc_prom(p0, self.prom)\n",
    "            p0 = torch.transpose(p0, 0, 1)\n",
    "            p0 = gaussian_transformation(p0)\n",
    "            p0 = self.ds_0_a(p0)\n",
    "            p0 = self.ds_0_b(p0)\n",
    "            p0, _ = torch.topk(p0, self.top_k, dim=0)\n",
    "\n",
    "            # same for p1\n",
    "            # print(f'{p1.max(), p1.min() = }')\n",
    "            p1 = torch.tensor(p1)*100 # not always with same len of p0\n",
    "            # print(f'{p1.max(), p1.min() = }')\n",
    "            p1_shape = p1.shape[1] # number of elemnts in H1 persistence\n",
    "            if p1_shape<self.top_k:\n",
    "                dim_to_add = self.top_k - p1_shape\n",
    "                aux_zeros = torch.zeros(p1.size(0), dim_to_add)\n",
    "                p1 = torch.cat((p1, aux_zeros), dim=1)\n",
    "\n",
    "            p1 = preproc_prom(p1, self.prom)\n",
    "            p1 = torch.transpose(p1, 0, 1)\n",
    "            p1 = gaussian_transformation(p1)\n",
    "            p1 = self.ds_1_a(p1)\n",
    "            p1 = self.ds_1_b(p1)\n",
    "            try:\n",
    "                p1, _ = torch.topk(p1, self.top_k, dim=0)\n",
    "            except RuntimeError:\n",
    "                raise ValueError('')\n",
    "            \n",
    "            if self.using_len_p1 == True:\n",
    "                emb_len = self.linear_dim_H1(torch.tensor([p1_shape]))\n",
    "                concat = torch.cat((p0.view(-1), p1.view(-1), emb_len.view(-1)))\n",
    "            else:\n",
    "                concat = torch.cat((p0.view(-1), p1.view(-1)))\n",
    "\n",
    "            labs = self.linear_labels(concat).unsqueeze(0)\n",
    "\n",
    "            if isinstance(labels,bool):\n",
    "                labels = labs\n",
    "            else:\n",
    "                labels = torch.cat((labels, labs), dim = 0)\n",
    "                \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, if_plot):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    target_labs = np.array([])\n",
    "    pred_labs = np.array([])\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # for batch in tqdm(test_data):\n",
    "        for batch in test_data:\n",
    "            batch_in_pd0 = [sample['persist_0'] for sample in batch] # get tersor of persistence\n",
    "            batch_in_pd1 = [sample['persist_1'] for sample in batch] # get tersor of persistence\n",
    "            batch_target = torch.tensor([sample['id_class'] for sample in batch]) # get target labels\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_in_pd0, batch_in_pd1)\n",
    "            # Get predicted labels\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # Total number of labels\n",
    "            total += batch_target.size(0)\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == batch_target).sum().item()\n",
    "            target_labs = np.append(target_labs, batch_target)\n",
    "            pred_labs = np.append(pred_labs, predicted)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    # print('Accuracy on the test set: {:.2f}%'.format(accuracy))\n",
    "\n",
    "    if if_plot:\n",
    "        cm = confusion_matrix(np.array(target_labs), np.array(pred_labs))\n",
    "        classes = [1,2,3,4,5]\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [04:01<00:00,  8.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, loss 33.020670508921015\n",
      "--> IMPROVEMENT from 0.0 to 69.93333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:57<00:00,  8.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, loss 20.68373385773978\n",
      "--> IMPROVEMENT from 69.93333333333334 to 71.53333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [04:02<00:00,  8.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, loss 18.348203870119885\n",
      "--> IMPROVEMENT from 71.53333333333333 to 73.33333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:54<00:00,  8.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, loss 17.603771232340065\n",
      "--> IMPROVEMENT from 73.33333333333333 to 74.26666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:55<00:00,  8.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, loss 17.290659854285096\n",
      "--> IMPROVEMENT from 74.26666666666667 to 74.86666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:55<00:00,  8.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, loss 16.912011134119332\n",
      "> Test Accuracy = 74.86666666666666 [best = 74.86666666666666]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:56<00:00,  8.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, loss 16.342555233053886\n",
      "--> IMPROVEMENT from 74.86666666666666 to 76.73333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:56<00:00,  8.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, loss 15.799397456784622\n",
      "--> IMPROVEMENT from 76.73333333333333 to 77.66666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:58<00:00,  8.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, loss 15.509009952675711\n",
      "--> IMPROVEMENT from 77.66666666666667 to 78.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:58<00:00,  8.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, loss 15.19938412157462\n",
      "--> IMPROVEMENT from 78.4 to 79.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [10:27<00:00, 22.41s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, loss 15.008400173156614\n",
      "--> IMPROVEMENT from 79.0 to 79.26666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "model = PersImage_KTH(hidden_size = 25)\n",
    "\n",
    "# Learning rate and loss function.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# scheduler = StepLR(optimizer, step_size=70, gamma=0.7)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 100\n",
    "recorded_loss = torch.zeros(epochs)\n",
    "\n",
    "best_acc = 0.0 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(train_batched_data):\n",
    "    # for batch in train_batched_data:\n",
    "\n",
    "        batch_in_pd0 = [sample['persist_0'] for sample in batch] # get tersor of persistence\n",
    "\n",
    "        batch_in_pd1 = [sample['persist_1'] for sample in batch] # get tersor of persistence\n",
    "\n",
    "        batch_target = torch.tensor([sample['id_class'] for sample in batch]) # get target labels\n",
    "        # print(f'\\n{len(batch_in_pd0) = }\\n{len(batch_in_pd1) = }\\n{batch_target.shape = }')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        result = model(batch_in_pd0, batch_in_pd1)\n",
    "        loss = loss_function(result, batch_target)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "    \n",
    "    recorded_loss[epoch] = total_loss\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, loss {total_loss}\")\n",
    "\n",
    "    test_acc = test_model(model, test_batched_data, if_plot=False)\n",
    "    if best_acc < test_acc:\n",
    "        if test_acc > 70.0:\n",
    "            torch.save(model.state_dict(), f'./Perslay_models/try_acc_{test_acc}.pth')\n",
    "        print(f'--> IMPROVEMENT from {best_acc} to {test_acc}')\n",
    "        best_acc = test_acc\n",
    "    else:\n",
    "        print(f'> Test Accuracy = {test_acc} [best = {best_acc}]')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recorded_loss)\n",
    "#ax.set_ylim([0, 1])\n",
    "plt.show()\n",
    "print(f\"Final loss is {recorded_loss[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = test_model(model, train_batched_data, if_plot=True); print(f'{train_acc = }')\n",
    "test_acc = test_model(model, test_batched_data, if_plot=True); print(f'{test_acc = }')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advtopo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
